{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYiOQG3XFlYX",
        "colab_type": "text"
      },
      "source": [
        "## chapter11. 심층신경망 훈련\n",
        "### 11.1. 그래디언트 소실과 폭주문제\n",
        "- gradient descent 훈련시 나타날 수 있는 문제점\n",
        "1. gradient vanishing : 알고리즘이 하위층으로 진행됨에 따라 그래디언트는 점차 작아져 하위층의 연결가중치를 실제 반영하지 못하는 경우 \n",
        "2. graidient exploding : 그래디언트가 점차 커져 여러개의 층이 비정상적으로 큰 가중치로 갱신되는 경우를 의미한다. \n",
        "- 원인 : gradient는 결국 미분값(=변화량)을 의미한다. 이 변화량이 작다면 network를 효과적으로 학습하지 못하게 되고, error-rate가 다 낮아지기 전에 다른 값으로 수렴하게 된다.   (ex) sigmoid에서 0이나 1로 수렴하는 경우 \n",
        "- 활성함수에서 0이나 1값으로 근사되어 버려, 초기 레이어에서 parameter-value의 큰 변화가 발생해도 output의 영향이 없어지는 문제점이다. (수식에 대한 관점의 설명은 아래의 bolg를 참고한다)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lXgekC09mBM",
        "colab_type": "text"
      },
      "source": [
        "- vanishing gradient, exploding gradient 설명 blog(수식참고) : https://ratsgo.github.io/deep%20learning/2017/10/10/RNNsty/\n",
        "- solution blog(필수) : https://wikidocs.net/61375\n",
        "- 기타 한국어 참고 설명: https://ydseo.tistory.com/41\n",
        "\n",
        "\n",
        "## 해결방안\n",
        "- 가중치 초기화 (세이비어 초기화, He초기화)\n",
        "- 수렴하지 않는 활성함수의 사용(ReLU, ReLU변형함수들)\n",
        "- 배치정규화(배치정규-> 결괏값 스케일 조정 후 이동)\n",
        "- 그래디언트 클리핑\n",
        "\n",
        "<br>\n",
        "\n",
        "### 11.1.1. 세이비어 초기화와 He초기화 (가중치 초기화 방법) \n",
        "- Xavier initializer : 시그모이드, 하이퍼볼릭 탄젠트 함수를 활성함수로 사용할 때 사용하는 가중치 초기화 방법. (논문: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
        "1. 균등분포로 초기화 : W~uni(-$sqrt(6/n_in+n_out)$, +$sqrt(6/n_in+n_out)$)\n",
        "2. 정규분포로 초기화 : W~N(0, $2/n_in+n_out$)\n",
        "\n",
        "<br>\n",
        "\n",
        "- He initializer : ReLU 함수를 활성함수로 사용할 때 사용하는 가중치 초기화 방법. (논문 : https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)\n",
        "1. 균등분포기반 초기화 :  W~uni(-$sqrt(6/n_in)$, +$sqrt(6/n_in)$)\n",
        "2. 정규분포기반 초기화 : W~N(0, $2/n_in$)\n",
        "\n",
        "<br>\n",
        "\n",
        "- 시그모이드 함수나 하이퍼볼릭탄젬트 함수를 활성함수로 사용할 때는 세이비어 초기화 방법을 사용하는 것이 효율적이다. \n",
        "- ReLU함수를 사용할 때는 He초기화 방식이 적절하다. \n",
        "- ReLU+He 초기화 방식을 사용하는 것이 일반적이다 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ9GEZ3WBlk3",
        "colab_type": "code",
        "outputId": "27faac65-21ff-41c1-a2a9-d30f00caf6f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "#idea: 완전 연결층을 생성하기 전에 초기화 먼저 시작한다. \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#데이터는 MNIST데이터를 사용한다. \n",
        "%tensorflow_version 1.x #버전지정\n",
        "import tensorflow as tf\n",
        "\n",
        "reset_graph()\n",
        "n_inputs=28*28 #뉴런의 수를 지정하는 경우 \n",
        "n_hidden1=300\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") #변수지정\n",
        "#변수 초기화 \n",
        "he_init=tf.variance_scaling_initializer()\n",
        "hidden1=tf.layers.dense(X,n_hidden1, activation=tf.nn.relu,\n",
        "                          kernel_initializer=he_init, name='hidden1', reuse=tf.AUTO_REUSE)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `1.x #버전지정`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:From <ipython-input-5-e7a404279b4b>:15: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5IvCfkIQbC5",
        "colab_type": "text"
      },
      "source": [
        "### 11.1.2. 수렴하지 않는 활성화함수 \n",
        "- 활성화함수를 잘못사용하면 그래디언트 폭주나 그래디언트 소실 문제로 이어질 수가 있다. \n",
        "- 은닉층에서 사용하는 활성화함수를 정할 필요가 있음.(가중치에 영향을 주는 함수는 결국 은닉층에서의 함수이므로)\n",
        "- 은닉층에서는 sigmoid를 사용하지 않는다\n",
        "- Leaky ReLU, RReLU, PReLE (변종 ReLU)를 사용하면 죽은 ReLU문제를 해결한다. \n",
        "- 혹은 ELU사용도 가능하다.혹은  SELU도 가능(계산속도는 느리다, 하지만 기존의 ReLU와는 다르케 z<0이어도 0으로 수렵하지 않는다. )\n",
        "- Leaky ReLU 논문 : https://arxiv.org/pdf/1505.00853.pdf\n",
        "- 은닉층에서 사용하는 활성함수 : ELU > Leaky ReLU > tanh > 로지스틱(시그모이드) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsYgOEyObFu7",
        "colab_type": "text"
      },
      "source": [
        "Leaky ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkiZWBu7bFFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#활성함수 정의 \n",
        "def leaky_relu(z, name=None):\n",
        "    return tf.maximum(0.01 * z, z, name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLt4bf0abaXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#그래프 구성\n",
        "reset_graph()\n",
        "\n",
        "X=tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "hidden1=tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiOBRlHRcK4s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "b296c042-a3c6-4310-e437-b171ceeb0d5a"
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
        "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaUG3M_vce0o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b75ca8ec-32cb-495b-80a0-c936f7672576"
      },
      "source": [
        "#데이터를 실제로 넣고 모델을 실제로 실행하는 단계\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
        "\n",
        "def shuffle_batch(X, y, batch_size):\n",
        "    rnd_idx = np.random.permutation(len(X))\n",
        "    n_batches = len(X) // batch_size\n",
        "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "        yield X_batch, y_batch\n",
        "\n",
        "n_epochs = 40\n",
        "batch_size = 50\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        if epoch % 5 == 0:\n",
        "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "0 Batch accuracy: 0.86 Validation accuracy: 0.904\n",
            "5 Batch accuracy: 0.94 Validation accuracy: 0.9496\n",
            "10 Batch accuracy: 0.92 Validation accuracy: 0.9652\n",
            "15 Batch accuracy: 0.94 Validation accuracy: 0.9706\n",
            "20 Batch accuracy: 1.0 Validation accuracy: 0.9762\n",
            "25 Batch accuracy: 1.0 Validation accuracy: 0.9776\n",
            "30 Batch accuracy: 0.98 Validation accuracy: 0.9782\n",
            "35 Batch accuracy: 1.0 Validation accuracy: 0.9786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddiaDqKxcrPT",
        "colab_type": "text"
      },
      "source": [
        "ELU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoMCO6zGMSHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#은닉층에 ELU사용하는 경우 \n",
        "#ELU정의 \n",
        "\n",
        "def elu(z, alpha=1):\n",
        "  return np.where(z<0, alpha*(np.exp(z)-1),z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMYRhk_gdaBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRJcek6-SfqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#elu를 은닉층의 활성함수로 사용하는 경우 \n",
        "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\", reuse=tf.AUTO_REUSE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zBZ2f4UTs0W",
        "colab_type": "text"
      },
      "source": [
        "### 11.1.3. 배치 정규화 \n",
        "- ReLU함수+He초기화를 사용해도 그래디언트 소실과 폭주가 일어날 수가 있다. 이에 따라 해줘야 하는 작업.\n",
        "- 배치정규화는 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만든다.(다만 각 배치샘플에 따라 정규화해줘야 하므로 '배치'정규화라 한다)\n",
        "- 배치정규화는 '내부공변량의 변화'에 의해 일어나는 현상이다.\n",
        "1. 활성함수 통화 전에 추가하는 연산(연산의 과정은 아래의 그림 참고)\n",
        "2. 층의 스케일 파라미터, 층의 이동 파라미터를 조정하여 작성된다. \n",
        "- 논문 : https://arxiv.org/pdf/1502.03167v3.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SNbSUGoxa8l",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/49298791/74802870-682b1680-531e-11ea-80fa-7013bea4885e.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8edmZgnTgFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#실행하기 전, 그래프 reset하는 방법\n",
        "import numpy as np\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC_-VXsayC04",
        "colab_type": "code",
        "outputId": "6f82e2a0-8dd3-4d9e-e161-55a27793bfc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "#배치 정규하 과정\n",
        "reset_graph() \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "n_inputs = 28 * 28\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
        "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
        "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
        "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
        "                                       momentum=0.9)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-cac275eafd54>:15: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6TAH2pnzQc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AGBJYbzzsD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#배치정규화의 매개변수의 반복을 없애기 위한 partial을 사용한다\n",
        "from functools import partial\n",
        "\n",
        "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
        "                              training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
        "bn1 = my_batch_norm_layer(hidden1) #단순한 배치 정규화를 partial로 저정\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
        "bn2 = my_batch_norm_layer(hidden2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "#출력층\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
        "logits = my_batch_norm_layer(logits_before_bn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72UTwVoH19kN",
        "colab_type": "code",
        "outputId": "c86b52be-8f93-4ac1-ccf7-44a3cd8a2b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "#BN 실행시 필요한 shuffle함수 \n",
        "reset_graph()\n",
        "def shuffle_batch(X, y, batch_size):\n",
        "    rnd_idx = np.random.permutation(len(X))\n",
        "    n_batches = len(X) // batch_size\n",
        "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "        yield X_batch, y_batch\n",
        "\n",
        "#데이터 로드 \n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
        "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    \n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT5LiPaJqSJQ",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "#실행단계\n",
        "#각 실행의 데이터마다 training의 placeholder를 사용해야 한다. \n",
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run([training_op, extra_update_ops],\n",
        "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"my_model_final.ckpt\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv5yjcvn47z5",
        "colab_type": "text"
      },
      "source": [
        "또한 비용을 최소화하는 파라미터를 찾을 수 있도록 saver를 지정하여 파라미터를 사용할 수가 있다. \n",
        "\n",
        "\n",
        "\n",
        "### 11.1.4. 그래디언트 클리핑\n",
        "- gradient의 최대갯수를 조정하고 graident가 최대치(threshold)를 넘게 되면 gradient의 크기를 재조정하여 gradient의 크기를 유지하는 방법이다. \n",
        "- 업데이트 하는 방법은 아래와 같다. \n",
        "- 논문 : http://proceedings.mlr.press/v28/pascanu13.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzvG828d79T-",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/49298791/74804869-d83c9b00-5324-11ea-99c6-72abc53e226a.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOCWRTgV9aVW",
        "colab_type": "text"
      },
      "source": [
        "그래디언트 클리핑 구성단계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Qce1BC8sQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#기본적인 layer구성에 대한 정의 \n",
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_hidden3 = 50\n",
        "n_hidden4 = 50\n",
        "n_hidden5 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
        "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
        "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV26JJ1U47Bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold=1.0 #하이퍼파라미터 튜닝 대상\n",
        "learning_rate=0.01\n",
        "\n",
        "optimizer= tf.train.GradientDescentOptimizer(learning_rate)\n",
        "grads_and_vars=optimizer.compute_gradients(loss) #비용함수 고려 \n",
        "capped_gvs=[(tf.clip_by_value(grad, -threshold, threshold), var)\n",
        "for grad, var in grads_and_vars]\n",
        "training_ops=optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wr5lsdO9fxP",
        "colab_type": "text"
      },
      "source": [
        "그래디언트 클리핑 실행단계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTLIdUqs9HPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#평가 \n",
        "with tf.name_scope(\"eval\"):\n",
        "  correct=tf.nn.in_top_k(logits,y,1) #top값을 가지는 경우 \n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Vt6mzS9uFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#실행단계 변수 초기화 \n",
        "init=tf.global_variables_initializer()\n",
        "saver=tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrvPX7TAEFwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "batch_norm_momentum = 0.9\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    he_init = tf.variance_scaling_initializer()\n",
        "\n",
        "    my_batch_norm_layer = partial(\n",
        "            tf.layers.batch_normalization,\n",
        "            training=training,\n",
        "            momentum=batch_norm_momentum)\n",
        "\n",
        "    my_dense_layer = partial(\n",
        "            tf.layers.dense,\n",
        "            kernel_initializer=he_init)\n",
        "\n",
        "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
        "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
        "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
        "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
        "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
        "    logits = my_batch_norm_layer(logits_before_bn)\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    \n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty8Ht7vJ91sU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs=20\n",
        "batch_size=200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcU68Edk94_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#세션 실행\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hROPXo7k6Ilv",
        "colab_type": "text"
      },
      "source": [
        "### 11.2, 미리 훈련된 층 재사용하기 \n",
        "- **전이학습** : 해결하려는 것과 비슷한 유형의 문제를 처리한 신경망이 있는지를 찾아보고 그 신경망의 하위층을 재사용하는 것. 훈련속도를 높이고 필요한 훈련데이터도 적다. \n",
        "- 지도학습의 경우, 전이학습의 방법은 기존의 학습된 모델에서 마지막 층을 제거한 후에 분류하고자 하는 층을 연결시켜주고 학습시킨다. \n",
        "- 전이 가능한 문제의 데이터는 많은데 전이 하려고 하는 문제의 데이터가 적을 때, 입력데이터가 같고 저레벨 특성의 문제를 해결하는데 도움이 될 때 사용한다. \n",
        "- (단 원래 사용한 것과 크기가 다른 이미지를 입력으로 사용한다면 원본 모델에 맞는 크기로 변경하는 전처리 단계를 거칠 필요가 있다. )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS9_u3LkXWh7",
        "colab_type": "text"
      },
      "source": [
        "#### 11.2.1. 텐서플로 모델 재사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuaSFVre-mGA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "edf49885-52ce-4990-c8a9-8d22060d85f2"
      },
      "source": [
        "%tensorflow_version 1.x #버전지정\n",
        "import tensorflow as tf\n",
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `1.x #버전지정`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRt0BEQ4qwQM",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "#앞에서 사용했던 그래프 그대로 가져온다\n",
        "reset_graph()\n",
        "saver=tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
        "\n",
        "#훈련 대상인 텐서를 직접 지정한다\n",
        "X=tf.get_default_graph.get_tensor_by_name(\"X:0\")\n",
        "y=tf.get_default_graph.get_tensor_by_name(\"y:0\")\n",
        "accuracy=tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
        "training_op=tf.get_default_graph().get_operation_by_name(\"GradientDescent\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOVErrrXd5Xq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e219f4e3-3c0e-4b1e-da63-c7c4a9706d05"
      },
      "source": [
        "#지금까지의 모든 연결된 연산리스트 확인\n",
        "for op in tf.get_default_graph().get_operations():\n",
        "    print(op.name)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X\n",
            "y\n",
            "hidden1/kernel/Initializer/random_uniform/shape\n",
            "hidden1/kernel/Initializer/random_uniform/min\n",
            "hidden1/kernel/Initializer/random_uniform/max\n",
            "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
            "hidden1/kernel/Initializer/random_uniform/sub\n",
            "hidden1/kernel/Initializer/random_uniform/mul\n",
            "hidden1/kernel/Initializer/random_uniform\n",
            "hidden1/kernel\n",
            "hidden1/kernel/Assign\n",
            "hidden1/kernel/read\n",
            "hidden1/bias/Initializer/zeros\n",
            "hidden1/bias\n",
            "hidden1/bias/Assign\n",
            "hidden1/bias/read\n",
            "dnn/hidden1/MatMul\n",
            "dnn/hidden1/BiasAdd\n",
            "dnn/hidden1/mul/x\n",
            "dnn/hidden1/mul\n",
            "dnn/hidden1/Maximum\n",
            "hidden2/kernel/Initializer/random_uniform/shape\n",
            "hidden2/kernel/Initializer/random_uniform/min\n",
            "hidden2/kernel/Initializer/random_uniform/max\n",
            "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
            "hidden2/kernel/Initializer/random_uniform/sub\n",
            "hidden2/kernel/Initializer/random_uniform/mul\n",
            "hidden2/kernel/Initializer/random_uniform\n",
            "hidden2/kernel\n",
            "hidden2/kernel/Assign\n",
            "hidden2/kernel/read\n",
            "hidden2/bias/Initializer/zeros\n",
            "hidden2/bias\n",
            "hidden2/bias/Assign\n",
            "hidden2/bias/read\n",
            "dnn/hidden2/MatMul\n",
            "dnn/hidden2/BiasAdd\n",
            "dnn/hidden2/mul/x\n",
            "dnn/hidden2/mul\n",
            "dnn/hidden2/Maximum\n",
            "outputs/kernel/Initializer/random_uniform/shape\n",
            "outputs/kernel/Initializer/random_uniform/min\n",
            "outputs/kernel/Initializer/random_uniform/max\n",
            "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
            "outputs/kernel/Initializer/random_uniform/sub\n",
            "outputs/kernel/Initializer/random_uniform/mul\n",
            "outputs/kernel/Initializer/random_uniform\n",
            "outputs/kernel\n",
            "outputs/kernel/Assign\n",
            "outputs/kernel/read\n",
            "outputs/bias/Initializer/zeros\n",
            "outputs/bias\n",
            "outputs/bias/Assign\n",
            "outputs/bias/read\n",
            "dnn/outputs/MatMul\n",
            "dnn/outputs/BiasAdd\n",
            "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
            "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
            "loss/Const\n",
            "loss/loss\n",
            "train/gradients/Shape\n",
            "train/gradients/grad_ys_0\n",
            "train/gradients/Fill\n",
            "train/gradients/loss/loss_grad/Reshape/shape\n",
            "train/gradients/loss/loss_grad/Reshape\n",
            "train/gradients/loss/loss_grad/Shape\n",
            "train/gradients/loss/loss_grad/Tile\n",
            "train/gradients/loss/loss_grad/Shape_1\n",
            "train/gradients/loss/loss_grad/Shape_2\n",
            "train/gradients/loss/loss_grad/Const\n",
            "train/gradients/loss/loss_grad/Prod\n",
            "train/gradients/loss/loss_grad/Const_1\n",
            "train/gradients/loss/loss_grad/Prod_1\n",
            "train/gradients/loss/loss_grad/Maximum/y\n",
            "train/gradients/loss/loss_grad/Maximum\n",
            "train/gradients/loss/loss_grad/floordiv\n",
            "train/gradients/loss/loss_grad/Cast\n",
            "train/gradients/loss/loss_grad/truediv\n",
            "train/gradients/zeros_like\n",
            "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
            "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
            "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
            "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
            "train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
            "train/gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
            "train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
            "train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
            "train/gradients/dnn/outputs/MatMul_grad/MatMul\n",
            "train/gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
            "train/gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
            "train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
            "train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
            "train/gradients/dnn/hidden2/Maximum_grad/Shape\n",
            "train/gradients/dnn/hidden2/Maximum_grad/Shape_1\n",
            "train/gradients/dnn/hidden2/Maximum_grad/Shape_2\n",
            "train/gradients/dnn/hidden2/Maximum_grad/zeros/Const\n",
            "train/gradients/dnn/hidden2/Maximum_grad/zeros\n",
            "train/gradients/dnn/hidden2/Maximum_grad/GreaterEqual\n",
            "train/gradients/dnn/hidden2/Maximum_grad/BroadcastGradientArgs\n",
            "train/gradients/dnn/hidden2/Maximum_grad/Select\n",
            "train/gradients/dnn/hidden2/Maximum_grad/Sum\n",
            "train/gradients/dnn/hidden2/Maximum_grad/Reshape\n",
            "train/gradients/dnn/hidden2/Maximum_grad/Select_1\n",
            "train/gradients/dnn/hidden2/Maximum_grad/Sum_1\n",
            "train/gradients/dnn/hidden2/Maximum_grad/Reshape_1\n",
            "train/gradients/dnn/hidden2/Maximum_grad/tuple/group_deps\n",
            "train/gradients/dnn/hidden2/Maximum_grad/tuple/control_dependency\n",
            "train/gradients/dnn/hidden2/Maximum_grad/tuple/control_dependency_1\n",
            "train/gradients/dnn/hidden2/mul_grad/Shape\n",
            "train/gradients/dnn/hidden2/mul_grad/Shape_1\n",
            "train/gradients/dnn/hidden2/mul_grad/BroadcastGradientArgs\n",
            "train/gradients/dnn/hidden2/mul_grad/Mul\n",
            "train/gradients/dnn/hidden2/mul_grad/Sum\n",
            "train/gradients/dnn/hidden2/mul_grad/Reshape\n",
            "train/gradients/dnn/hidden2/mul_grad/Mul_1\n",
            "train/gradients/dnn/hidden2/mul_grad/Sum_1\n",
            "train/gradients/dnn/hidden2/mul_grad/Reshape_1\n",
            "train/gradients/dnn/hidden2/mul_grad/tuple/group_deps\n",
            "train/gradients/dnn/hidden2/mul_grad/tuple/control_dependency\n",
            "train/gradients/dnn/hidden2/mul_grad/tuple/control_dependency_1\n",
            "train/gradients/AddN\n",
            "train/gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
            "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
            "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
            "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
            "train/gradients/dnn/hidden2/MatMul_grad/MatMul\n",
            "train/gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
            "train/gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
            "train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
            "train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
            "train/gradients/dnn/hidden1/Maximum_grad/Shape\n",
            "train/gradients/dnn/hidden1/Maximum_grad/Shape_1\n",
            "train/gradients/dnn/hidden1/Maximum_grad/Shape_2\n",
            "train/gradients/dnn/hidden1/Maximum_grad/zeros/Const\n",
            "train/gradients/dnn/hidden1/Maximum_grad/zeros\n",
            "train/gradients/dnn/hidden1/Maximum_grad/GreaterEqual\n",
            "train/gradients/dnn/hidden1/Maximum_grad/BroadcastGradientArgs\n",
            "train/gradients/dnn/hidden1/Maximum_grad/Select\n",
            "train/gradients/dnn/hidden1/Maximum_grad/Sum\n",
            "train/gradients/dnn/hidden1/Maximum_grad/Reshape\n",
            "train/gradients/dnn/hidden1/Maximum_grad/Select_1\n",
            "train/gradients/dnn/hidden1/Maximum_grad/Sum_1\n",
            "train/gradients/dnn/hidden1/Maximum_grad/Reshape_1\n",
            "train/gradients/dnn/hidden1/Maximum_grad/tuple/group_deps\n",
            "train/gradients/dnn/hidden1/Maximum_grad/tuple/control_dependency\n",
            "train/gradients/dnn/hidden1/Maximum_grad/tuple/control_dependency_1\n",
            "train/gradients/dnn/hidden1/mul_grad/Shape\n",
            "train/gradients/dnn/hidden1/mul_grad/Shape_1\n",
            "train/gradients/dnn/hidden1/mul_grad/BroadcastGradientArgs\n",
            "train/gradients/dnn/hidden1/mul_grad/Mul\n",
            "train/gradients/dnn/hidden1/mul_grad/Sum\n",
            "train/gradients/dnn/hidden1/mul_grad/Reshape\n",
            "train/gradients/dnn/hidden1/mul_grad/Mul_1\n",
            "train/gradients/dnn/hidden1/mul_grad/Sum_1\n",
            "train/gradients/dnn/hidden1/mul_grad/Reshape_1\n",
            "train/gradients/dnn/hidden1/mul_grad/tuple/group_deps\n",
            "train/gradients/dnn/hidden1/mul_grad/tuple/control_dependency\n",
            "train/gradients/dnn/hidden1/mul_grad/tuple/control_dependency_1\n",
            "train/gradients/AddN_1\n",
            "train/gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
            "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
            "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
            "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
            "train/gradients/dnn/hidden1/MatMul_grad/MatMul\n",
            "train/gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
            "train/gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
            "train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
            "train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
            "train/GradientDescent/learning_rate\n",
            "train/GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
            "train/GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
            "train/GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
            "train/GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
            "train/GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
            "train/GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
            "train/GradientDescent\n",
            "eval/in_top_k/InTopKV2/k\n",
            "eval/in_top_k/InTopKV2\n",
            "eval/Cast\n",
            "eval/Const\n",
            "eval/Mean\n",
            "init\n",
            "save/filename/input\n",
            "save/filename\n",
            "save/Const\n",
            "save/SaveV2/tensor_names\n",
            "save/SaveV2/shape_and_slices\n",
            "save/SaveV2\n",
            "save/control_dependency\n",
            "save/RestoreV2/tensor_names\n",
            "save/RestoreV2/shape_and_slices\n",
            "save/RestoreV2\n",
            "save/Assign\n",
            "save/Assign_1\n",
            "save/Assign_2\n",
            "save/Assign_3\n",
            "save/Assign_4\n",
            "save/Assign_5\n",
            "save/restore_all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_viwyWiszcJ",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "#원본 모델에 대하여 문서화한다. \n",
        "for op in (X,y,accuracy, training_op):\n",
        "  tf.add_to_collection(\"my_training_op\", op)\n",
        "#위와 같이 저장한 모델을 재사용할 때는 \n",
        "X,y,accuracy,training_op=tf.get_collection(\"my_training_op\")\n",
        "\n",
        "#미리 훈련된 층 위에 새로운 모델을 훈련하는 경우 \n",
        "#출력에 대한 손실을 계산하고 이를 최소화하기 위한 옵티마이저의 사용\n",
        "reuse_vars=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\")\n",
        "restore_saver=tf.train.Saver(reuse_vars) #복원\n",
        "init=tf.global_variables_initializer() #변수 초기화 \n",
        "saver=tf.train.Saver() #새로운 모델저장\n",
        "\n",
        "#실행\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  restore_saver.restore(sess,\"./my_model_final.ckpt\")\n",
        "  save_path=sess.saver(sess,\"./my_model_final.ckpt\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H13Yw_B7wAah",
        "colab_type": "text"
      },
      "source": [
        "#### 11.2.2.다른 프레임워크 모델 재사용하기 \n",
        "- 만약 모델이 다른 프레임워클 훈련되어 있다면 수동으로 모델 파라미터를 읽어 들여 적절한 변수에 할당해야 한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKcg9bfX9SjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#원본모델이 tensorflow로 학습되었으면 바로 불러와서 사용가능\n",
        "#import_meta_graph로 기존 연산에 적재한다. \n",
        "reset_graph()\n",
        "original_w=[[1.,2.,3.], [4.,5.,6.]] #기존의 가중치\n",
        "original_b=[7.,8.,9.] #기존의 bias\n",
        "\n",
        "X=tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "hidden1=tf.layers.dense(X,n_hidden1, activation=tf.nn.relu, name=\"hidden1\",reuse=tf.AUTO_REUSE)\n",
        "\n",
        "graph=tf.get_default_graph()\n",
        "assign_kernel=graph.get_operation_by_name(\"hidden1/kernel/Assign\") #선언한 노드이름으로 불러오는 과정\n",
        "assign_bias=graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
        "init_kernel= assign_kernel.inputs[1]\n",
        "init_bias=assign_bias.inputs[1]\n",
        "\n",
        "init=tf.global_variables_initializer() #graph변수 초기화 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT_W1-sIw12f",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "#graph실행 세션\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init, feed_dict={init_kernel:original_w, init_bias: original_b})\n",
        "  print(hidden1.eval(feed_dict={X:[[10.0, 11.0]]})) #각 세션 layer ReLU(weight+bias)결과 확인\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRXYQOIJEUps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bcb393c4-e356-418a-8954-d341bc5563c7"
      },
      "source": [
        "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'hidden1/kernel:0' shape=(784, 300) dtype=float32_ref>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scn7cUQCoFPd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2bf3ce1f-674e-4a8d-fb8e-de3e5189a435"
      },
      "source": [
        "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'hidden1/bias:0' shape=(300,) dtype=float32_ref>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t-DPv1ixP68",
        "colab_type": "text"
      },
      "source": [
        "#### 11.2.3. 신경망의 하위층을 학습에서 제외하기 \n",
        "- 첫번째 DNN의 하위층은 이미 저수준의 특성을 감지하도록 학습되어 있기 때문에 다른 이미지 분류 작업에 유용하다\n",
        "- 일반적으로 새로운 DNN을 훈련시킬 때 재사용되는 층들의 가중치를 동결하는 것이 좋다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoJ106fu1KKh",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "#은닉층 3,4층과 출력층에 있는 학습할 변수 목록 저장\n",
        "train_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|outputs\")\n",
        "#훈련에 사용할 변수를 동결하는 방법 1\n",
        "training_op=optimizer.minimize(loss, var_list=train_vars)\n",
        "#훈련데 사용할 변수를 동결하는 방법 2\n",
        "with tf.name_scope(\"dnn\"):\n",
        "  hidden1=tf.layers.dense(X,n_hidden1, activation=tf.nn.relu, name=\"hidden\")\n",
        "  hidden2=tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
        "  hidden3=tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
        "  hidden4=tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, nam=\"hidden4)\n",
        "  logits=tf.layers.dense(hidden4, n_outputs, name=\"outputs\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEsRRn7P2Mal",
        "colab_type": "text"
      },
      "source": [
        "#### 11.2.4. 동결된 층 캐싱하기 \n",
        "- 캐싱 : 일반적인 특징이 있는 데이터의 하위 집합을 저장하는 고속 데이터 스토리지를 의미한다. \n",
        "- 동결된 층은 변하지 않기 때문에 각 훈련 샘플에 대해 가장 위쪽의 동결된 층에서 나온 출력을 캐싱하는 것이 가능하다. \n",
        "- 이렇게 만들어 버리면 동결된 층을 지날 때 학습속도가 훨씬 좋아진다. \n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "n_batches=len(X_train) #batch_size\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  restore_saver.restore(sess, './my_model_final.ckpt')\n",
        "  h2_cache=sess.run(hidden2, feed_dict={X:x_train})\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    shuffled_idx=np.random.permutation(len(X_train))\n",
        "    hidden2_batches=np.array_split(h2_cache[shuffled_idx], n_batches)\n",
        "    y_batches=np.array_split(y_train[shuffled_idx], n_batches)\n",
        "    for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
        "      sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
        "  save_path=saver.save(sess, \",/my_new_model_final.ckpt\")\n",
        "#두번째 은닉층의 출력을 배치로 만들어 주입하는 경우이다. \n",
        "```\n",
        "\n",
        "#### 11.2.5. 상위층을 변경, 삭제, 대체하기 \n",
        "- 원본모델에서의 상위층은 대체로 덜 유용하므로 거의 대체된다고 본다. \n",
        "- 그래서 재사용할 적절한 층의 개수를 아는 것이 중요하다\n",
        "- **적절한 재사용층 아는 방법** : \n",
        "  1. 복사한 층을 동결 후 훈련하여 성능을 평가 \n",
        "  2. 위쪽의 은닉층을 동결 해제하면서 가중치 변경하여 성능이 좋아지는지를 확인\n",
        "  3. 그래도 좋은 성능을 얻지 못하면 가장 위쪽의 은닉층을 제거하고 남은 은닉층을 동결한다. \n",
        "\n",
        "#### 11.2.6. 모델 저장소\n",
        "- 당면한 문제와 비슷한 작업을 훈련시킨 신경망을 찾을 수 있는 곳\n",
        "  1. 자신의 모델 카탈로그\n",
        "  2. 텐서플로우에서 제공하는 모델 저장소 : https://github.com/tensorflow/models\n",
        "  3. 카페 모델 저장소 : https://goo.gl/XI02X3\n",
        "\n",
        "\n",
        "#### 11.2.7. 비지도 사전훈련\n",
        "- 훈련 데이터가 부족한 경우에 전이학습을 통해 해결한다고 앞에서 언급했음. 하지만 비슷한 작업에 대한 훈련된 모델을 찾을 수가 없다면?\n",
        "  1. 더 많은 훈련 데이터를 찾는다\n",
        "  2. 비지도 사전훈련을 하는 경우 : 오토인코더, 제한된 볼트만 머신\n",
        "- 모든 층이 훈련되면 지도학습으로 신경망을 세밀하게 튜닝이 가능하다. (375p그림)\n",
        "\n",
        "#### 11.2.8. 보조작업으로 사전훈련\n",
        "- 레이블된 훈련 데이터를 쉽게 얻거나 생성할 수 있는 보조 작업에 첫번째 신경망을 훈련하는 것이다. \n",
        "- 신경망의 하위층을 실제 작업을 위해 재사용하는 것이다. \n",
        "- ex. image detection   : 하나의 이미지로 iteration에 따라 다르게 학습시키는 경우 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyU6ufZE3ewr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}