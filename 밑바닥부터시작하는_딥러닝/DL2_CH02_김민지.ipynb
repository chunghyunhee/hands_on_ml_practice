{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL2_CH02_김민지.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4qF0oDW_iR8",
        "colab_type": "text"
      },
      "source": [
        "# **Chapter 2 자연어와 단어의 분산 표현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFrc8382_ngN",
        "colab_type": "text"
      },
      "source": [
        "## **2.1 자연어 처리란**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDwBW2Yk_qTK",
        "colab_type": "text"
      },
      "source": [
        "한국어와 영어 등 우리가 평소에 쓰는 말을 자연어라고 한다. 자연어 처리란 자연어를 처리하는 분야로, 알기 쉽게 풀어보면 '우리의 말을 컴퓨터에게 이해시키기 위한 기술(분야)'이다. 자연어 처리 기술이 사용되는 예로 검색 엔진, 기계 번역, 질의응답 시스템, IME(입력기 전환), 문장 자동요약과 감정분석 등이 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTn-tAvP__Ku",
        "colab_type": "text"
      },
      "source": [
        "### **2.1.1 단어의 의미**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUZzmXTdACWi",
        "colab_type": "text"
      },
      "source": [
        "단어는 의미의 최소 단위이다. 따라서 자연어를 컴퓨터에게 이해시키는 데는 무엇보다 '단어의 의미'를 이해시키는 게 중요하다. 2장에서는 사람의 손으로 만든 시소러스(유의어 사전)를 이용하는 방법과 통계 정보로부터 단어를 표현하는 '통계 기반 기법'을 설명한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QEZjZNLATE3",
        "colab_type": "text"
      },
      "source": [
        "## **2.2 시소러스**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pcFH2ZCAVQf",
        "colab_type": "text"
      },
      "source": [
        "자연어 처리의 역사를 되돌아보면 단어의 의미를 인력을 동원해 정의하려는 시도는 수없이 있어왔다. 그러나 사람이 이용하는 일반적인 사전이 아니라 시소러스 형태의 사전을 애용했다. 시소러스란 유의어 사전으로, '뜻이 같은 단어(동의어)'나 '뜻이 비슷한 단어(유의어)'가 한 그룹으로 분류되어 있다. 또한 자연어 처리에 이용되는 시소러스에서는 단어 사이의 '상위와 하위' 혹은 '전체와 부분' 등, 더 세세한 관계까지 정의해둔 경우가 있다. \n",
        "\n",
        "모든 단어에 대한 유의어 집합을 만든 다음, 단어들의 관계를 그래프로 표현하여 단어 사이의 연결을 정의할 수 있다. 그러면 이 단어 네트워크를 이용하여 컴퓨터에게 단어 사이의 관계를 가르칠 수도 있다. \n",
        "\n",
        "시소러스를 사용하는 예로는 검색 엔진이 있다. automobile과 car이 유의어임을 알고 있으면 car의 검색 결과에 automobile의 검색 결과도 포함시켜주면 좋을 것이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv1Q-zIDBHkN",
        "colab_type": "text"
      },
      "source": [
        "### **2.2.1 WordNet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_phy21Q-BKB3",
        "colab_type": "text"
      },
      "source": [
        "자연어 처리 분야에서 가장 유명한 시소러스는 WordNet이 있따. WordNet을 사용하면 유의어를 얻거나 단어 네트워크를 이용할 수 있다. 또한 단어 네트워크를 사용해 단어 사이의 유사도를 구할 수도 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ItwVrVOBUd8",
        "colab_type": "text"
      },
      "source": [
        "### **2.2.2 시소러스의 문제점**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAhDSqnMBX5M",
        "colab_type": "text"
      },
      "source": [
        "WordNet과 같은 시소러스에는 수많은 단어에 대한 동의어와 계층 구조 등의 관계가 정의돼있다. 하지만 이처럼 사람이 수작업으로 레이블링하는 방식에는 큰 결저이 존재한다. \n",
        "\n",
        " 1. 시대 변화에 대응하기 어렵다. 단어의 변화에 대응하기 위해서는 시소러스를 사람이 수작업으로 끊임없이 갱신해야 한다는 번거로움이 있다. \n",
        " 2. 사람을 쓰는 비용은 크다. 따라서 시소러스를 만드는 데 엄청난 인적 비용이 발생한다. \n",
        " 3. 단어의 미묘한 차이를 표현할 수 없다. 사소한 용법 차이 등을 수작업으로 표현하는 것은 상당히 곤란한 일이다. \n",
        "\n",
        "이 문제를 피하기 위해 사용하는 것이 통계 기반 기법과 신경망을 사용한 추론 기반 기법이다. 이 두 기법에서는 대량의 텍스트 데이터로부터 단어의 의미를 자동으로 추출한다. 사람이 수작업으로 시소러스나 관계(특징)를 설계하던 방식으로부터 사람의 개입을 최소로 줄이고 텍스트 데이터만으로 원하는 결과를 얻어내는 방향으로 패러다임이 바뀌고 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrStu6DzDSv4",
        "colab_type": "text"
      },
      "source": [
        "## **2.3 통계 기반 기법**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBvh6qCrDhk_",
        "colab_type": "text"
      },
      "source": [
        "말뭉치란 맹목적으로 수집된 텍스트 데이터가 아닌 자연어 처리 연구나 애플리케이션을 염두에 두고 수집된 대량의 텍스트 데이터를 말한다. 통계 기반 기법의 목표는 이처럼 사람의 지식으로 가득한 말뭉치에서 자동으로, 그리고 효율적으로 그 핵심을 추출하는 것이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MGeorV7DwyQ",
        "colab_type": "text"
      },
      "source": [
        "### **2.3.1 파이썬으로 말뭉치 전처리하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fUFFScaEDL-",
        "colab_type": "text"
      },
      "source": [
        "자연어 처리에는 위키백과와 구글 뉴스, 대문호의 작품들과 같이 다양한 말뭉치가 사용된다. 먼저 파이썬의 대화 모드를 이용하여 매우 작은 텍스트 데이터에 전처리를 해볼 것이다. 여기서 말하는 전처리는 텍스트 데이터를 단어로 분할하고 그 분할된 단어들을 단어 ID 목록으로 변환하는 일이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86osOXP__UNP",
        "colab_type": "code",
        "outputId": "e6e677a4-63e1-43ea-8efc-6ce070c66978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "text = text.lower()\n",
        "text = text.replace('.',' .')\n",
        "text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you say goodbye and i say hello .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdxqEqjAE2y8",
        "colab_type": "text"
      },
      "source": [
        "간단한 예시 문장 하나를 text 변수에 저장했다.\n",
        "\n",
        "lower() 메서드로 모든 문자를 소문자로 변환했다. 이후에 split() 메서드를 이용해 공백을 기준으로 분할해주기 위해 문장 끝의 마침표를 앞에 공백이 있는 마침표로 변환했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-76_OuWJEvpN",
        "colab_type": "code",
        "outputId": "5498c53f-8d66-4b27-e096-54e331806ae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "words = text.split(' ')\n",
        "words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf86XTxNEkZ5",
        "colab_type": "text"
      },
      "source": [
        "split() 메서드를 이용해 text를 단어 단위로 분할했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62_u6PkyEfGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_id = {}\n",
        "id_to_word = {}\n",
        "\n",
        "for word in words:\n",
        "  if word not in word_to_id:\n",
        "    new_id = len(word_to_id)\n",
        "    word_to_id[word] = new_id\n",
        "    id_to_word[new_id] = word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bifv8ewXFonm",
        "colab_type": "text"
      },
      "source": [
        "단어를 텍스트 그대로 조작하는 것은 불편하기 때문에, 단어에 ID를 부여하고 ID의 리스트로 이용할 수 있도록 손질했다. 이를 위한 사전 준비로, 파이썬의 딕셔너리를 이용하여 단어 ID와 단어를 짝지어주는 대응표를 작성했다. \n",
        "\n",
        "단어 ID에서 단어로의 변환은 id_to_word가 담당하며, 단어에서 단어 ID로의 변환은 word_to_id가 담당한다. \n",
        "\n",
        "for문은 단어 단위로 분할된 words의 각 원소를 처음부터 하나씩 살펴보면서, 단어가 word_to_id에 들어 있지 않으면 word_to_id와 id_to_word 각각에 새로운 ID와 단어를 추가하도록 구현했다. 또한 추가 시점의 딕셔너리 길이가 새로운 단어의 ID로 설정되기 때문에 단어 ID는 0,1,2,... 식으로 증가한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPmY1g_eFnxP",
        "colab_type": "code",
        "outputId": "b5dcf2ad-10e2-49b1-ded8-aa3a6dd39838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "id_to_word"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs0wO4mbGjk_",
        "colab_type": "code",
        "outputId": "900eacbe-1433-4117-98d7-6c85f548d561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "word_to_id"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 6, 'and': 3, 'goodbye': 2, 'hello': 5, 'i': 4, 'say': 1, 'you': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YejlaJt4GouO",
        "colab_type": "text"
      },
      "source": [
        "단어 ID와 단어의 대응표에 실제로 어떤 내용이 담겨 있는지 확인했다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMogm6hZGldt",
        "colab_type": "code",
        "outputId": "50e34901-26f8-4e92-a391-7e34df722577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "id_to_word[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'say'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO4lPJOhGygp",
        "colab_type": "code",
        "outputId": "7742afec-f620-4107-8adf-2c7e094ca949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "word_to_id['hello']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLUJj-9YG2l5",
        "colab_type": "text"
      },
      "source": [
        "이처럼 딕셔너리를 사용하면 단어를 가지고 단어 ID를 검색하거나, 반대로 단어 ID를 가지고 단어를 검색할 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbO9OM5FG1lJ",
        "colab_type": "code",
        "outputId": "bc7f922e-5e02-4619-c7d4-046290364267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import numpy as np\n",
        "corpus = [word_to_id[w] for w in words]\n",
        "corpus = np.array(corpus)\n",
        "corpus"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 1, 5, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJM0ZtzKHJNa",
        "colab_type": "text"
      },
      "source": [
        "단어 목록을 단어 ID 목록으로 변경해보았다. \n",
        "\n",
        "먼저 파이썬의 내포 표기를 사용하여 단어 목록에서 단어 ID 목록으로 변환한 다음, 다시 넘파이 배열로 변환했다. (여기서 내포란, 리스트나 딕셔너리 등의 반복문 처리를 간단하게 쓰기 위한 기법이다. 예를 들어 xs = [1,2,3,4]라는 리스트의 각 원소를 제곱하여 새로운 리스트를 만들고 싶다면 [x**2 for x in xs] 처럼 된다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvaR2h6nHHMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  text = text.replace('.',' .')\n",
        "  words = text.split(' ')\n",
        "  \n",
        "  word_to_id = {}\n",
        "  id_to_word = {}\n",
        "  for word in words:\n",
        "    if word not in word_to_id:\n",
        "      new_id = len(word_to_id)\n",
        "      word_to_id[word] = new_id\n",
        "      id_to_word[new_id] = word\n",
        "\n",
        "  corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "  return corpus, word_to_id, id_to_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18RJryE6Itvu",
        "colab_type": "text"
      },
      "source": [
        "지금까지의 처리를 한 데 모아 preprocess() 함수로 구현해주었다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjVyCybIIWbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus,word_to_id,id_to_word = preprocess(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN6Gj66BJDMt",
        "colab_type": "text"
      },
      "source": [
        "이 함수를 사용하여 말뭉치 전처리를 수행해주었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7alj7GEdJB8e",
        "colab_type": "code",
        "outputId": "922d3225-94ec-4b78-bf49-c5515c4dddd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "print(preprocess(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([0, 1, 2, 3, 4, 1, 5, 6]), {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}, {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y73_Ee4NJMTj",
        "colab_type": "code",
        "outputId": "9fb764c0-213a-4e9f-8a4e-0ebaec658f51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 1 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP4tQhS_JOar",
        "colab_type": "code",
        "outputId": "507c1da7-f426-4ff4-952d-5e1e6eab7b75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(word_to_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfrfyNiFJPpB",
        "colab_type": "code",
        "outputId": "53401b03-605a-4722-ad52-1180bc947250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(id_to_word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daNn5FnyJalF",
        "colab_type": "text"
      },
      "source": [
        "전처리가 제대로 수행되었음을 확인했다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmcDqt_GKI7r",
        "colab_type": "text"
      },
      "source": [
        "### **2.3.2 단어의 분산 표현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awbgZmgKKL6E",
        "colab_type": "text"
      },
      "source": [
        "색을 표현할 때 고유한 이름을 붙여 표현할 수도 있지만 RGB라는 세 가지 성분이 어떤 비율로 섞여 있느냐로 표현할 수도 있다. 즉 색을 3차원의 벡터로 표현한다는 것인데, 이 방법이 색을 더 정확하게 명시할 수 있다. 모든 색을 단 3개의 성분으로 표현할 수 있으니 간결하고, 어떤 색인지 짐작하기도 쉬우며, 색끼리의 관련성도 쉽게 판단할 수 있다. 또한 정량화하기도 쉽다. \n",
        "\n",
        "색을 벡터로 표현할 수 있듯 단어 또한 벡터로 표현할 수 있는데, 이를 자연어 처리 분야에서는 단어의 분산 표현이라고 한다. 단어의 분산 표현은 단어를 고정 길이의 밀집벡터로 표현한다. 밀집벡터라 함은 대부분의 원소가 0이 아닌 실수인 벡터를 말한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nAZPUB6LHQv",
        "colab_type": "text"
      },
      "source": [
        "### **2.3.3 분포 가설**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWOqYu9cLM8O",
        "colab_type": "text"
      },
      "source": [
        "단어를 벡터로 표현하는 중요한 기법들은 거의 모두 분포 가설('단어의 의미는 주변 단어에 의해 형성된다'는 아이디어)에 기초한다. 분포 가설이 말하고자 하는 바는 단어 자체에는 의미가 없고, 그 단어가 사용된 '맥락'이 의미를 형성한다는 것이다. 이번 장에서 '맥락'이라고 하면 주목하는 단어 주변에 놓인 단어를 가리킨다. 맥락의 크기(주변 단어를 몇 개나 포함할지)를 '윈도우 크기'라고 한다. 윈도우 크기가 1이면 좌우 한 단어씩이, 윈도우 크기가 2이면 좌우 두 단어씩이 맥락에 포함된다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr-DIKbZL7rH",
        "colab_type": "text"
      },
      "source": [
        "### **2.3.4 동시발생 행렬**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov7nrK3KMTzJ",
        "colab_type": "text"
      },
      "source": [
        "통계 기반 기법이란 어떤 단어에 주목했을 때, 그 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방법이다. 이는 분포 가설에 기초하는 방법이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b5tKXnrJQs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.util import preprocess\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tghm9hPM-D9",
        "colab_type": "code",
        "outputId": "c4817c9c-f36a-43e9-ceac-e33ae0603e6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 1 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z6zUOthNN86",
        "colab_type": "code",
        "outputId": "eac9c4a0-7c6b-48c6-9a50-a8f0fb0244e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(id_to_word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ-v8woGNSBX",
        "colab_type": "text"
      },
      "source": [
        "먼저 preprocess() 함수를 사용해 전처리를 해주었다. 결과를 확인해보면 단어 수가 총 7개임을 알 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_ajE_nzNnld",
        "colab_type": "text"
      },
      "source": [
        "모든 단어에 대해 동시발생하는 단어를 표에 정리하면 행렬의 형태를 띠게 되는데, 이를 '동시발생 행렬'이라고 한다. 동시발생 행렬의 각 행은 해당 단어를 표현한 벡터가 된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOxHmX3KOAwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C = np.array([\n",
        "   [0,1,0,0,0,0,0],\n",
        "   [1,0,1,0,1,1,0],\n",
        "   [0,1,0,1,0,0,0],\n",
        "   [0,0,1,0,1,0,0],\n",
        "   [0,1,0,1,0,0,0],\n",
        "   [0,1,0,0,0,0,1],\n",
        "   [0,0,0,0,0,1,0],\n",
        "],dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh8l54zlOfhx",
        "colab_type": "text"
      },
      "source": [
        "교재 p.90 그림 2-7의 동시발생 행렬을 수동으로 구현했다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0A3LT-rOc_z",
        "colab_type": "code",
        "outputId": "de2d878b-3a05-45b0-eeed-873e971dc337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(C[0]) # ID가 0인 단어의 벡터 표현"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSYwbRfYOsR8",
        "colab_type": "code",
        "outputId": "1926899d-7da7-44a5-8099-98b813028bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(C[4]) # ID가 4인 단어의 벡터 표현"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 1 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgokLRfzOylz",
        "colab_type": "code",
        "outputId": "6a71ee6f-3e4a-482e-8bf2-3bc95a4c9dc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(C[word_to_id['goodbye']]) # \"goodbye\"의 벡터 표현"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 1 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKpgAHo9O-Kl",
        "colab_type": "text"
      },
      "source": [
        "동시발생 행렬을 사용하면 위와 같이 각 단어의 벡터를 얻을 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdIKge4mO9ab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
        "  corpus_size = len(corpus)\n",
        "  co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "  for idx, word_id in enumerate(corpus):\n",
        "    for i in range(1, window_size + 1):\n",
        "      left_idx = idx - i\n",
        "      right_idx = idx + i \n",
        "\n",
        "      if left_idx >= 0:\n",
        "        left_word_id = corpus[left_idx]\n",
        "        co_matrix[word_id, left_word_id] += 1\n",
        "\n",
        "      if right_idx < corpus_size:\n",
        "        right_word_id = corpus[right_idx]\n",
        "        co_matrix[word_id, right_word_id] += 1\n",
        "\n",
        "    return co_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj1zu-H3Qwub",
        "colab_type": "text"
      },
      "source": [
        "동시발생 행렬을 자동화하여 구현해보았다. creat_co_matrix() 함수의 인수들은 차례로 단어 ID의 리스트, 어휘 수, 윈도우 크기를 나타낸다. \n",
        "\n",
        "이 함수는 먼저 co_matrix를 0으로 채워진 2차원 배열로 초기화한다. 그 다음은 말뭉치의 모든 단어 각각에 대하여 윈도우에 포함된 주변 단어를 세어나간다. 이 때 말뭉치의 왼쪽 끝과 오른쪽 경계를 벗어나지 않는지도 확인한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0Zu4BqaRgio",
        "colab_type": "text"
      },
      "source": [
        "### **2.3.5 벡터 간 유사도**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYLYtqUBRjFj",
        "colab_type": "text"
      },
      "source": [
        "벡터 간 유사도를 측정할 때는 벡터의 내적, 유클리드 거리 등을 이용한다. 단어 벡터의 유사도를 나타낼 때는 코사인 유사도를 자주 이용한다. (코사인 유사도의 정의는 교재 p.92를 참고한다.) 코사인 유사도를 직관적으로 풀어보면 '두 벡터가 가리키는 방향이 얼마나 비슷한가'이다. 두 벡터의 방향이 완전히 같다면 코사인 유사도가 1이 되며, 완전히 반대라면 -1이 된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbwHq05aQv-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cos_similarity(x,y):\n",
        "  nx = x / np.sqrt(np.sum(x**2)) # x의 정규화\n",
        "  ny = y / np.sqrt(np.sum(y**2)) # y의 정규화\n",
        "  return np.dot(nx,ny)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su6uCsLLUwBN",
        "colab_type": "text"
      },
      "source": [
        "코사인 유사도를 코드로 구현해보았다. \n",
        "\n",
        "이 코드에서 인수 x와 y는 넘파이 배열이라고 가정한다. 이 함수는 먼저 벡터 x와 y를 정규화한 후 두 벡터의 내적을 구했다. 그런데 이 구현에는 문제가 하나 있다. 인수로 제로 벡터(원소가 모두 0인 벡터)가 들어오면 0으로 나누게 되는 오류가 발생한다. 이 문제를 해결하는 전통적인 방법은 나눌 때 분모에 작은 값을 더해주는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvJJjh0cUg91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cos_similarity(x,y,eps=1e-8):\n",
        "  nx = x / (np.sqrt(np.sum(x**2)) + eps)\n",
        "  ny = y / (np.sqrt(np.sum(y**2)) + eps)\n",
        "  return np.dot(nx,ny)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD-KcUHAVXUR",
        "colab_type": "text"
      },
      "source": [
        "작은 값을 뜻하는 eps를 인수로 받도록 하고, 이 인수의 값을 지정하지 않으면 기본값으로 1e-8(=0.00000001)이 설정되도록 수정하였다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8F_Gli-VWO-",
        "colab_type": "code",
        "outputId": "165283e5-209d-4168-908a-1d5e5aea6940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.util import preprocess, create_co_matrix, cos_similarity\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus,vocab_size)\n",
        "\n",
        "c0 = C[word_to_id['you']] # \"you\"의 단어 벡터\n",
        "c1 = C[word_to_id['i']] # \"i\"의 단어 벡터\n",
        "print(cos_similarity(c0,c1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7071067691154799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLTK-43GWUC_",
        "colab_type": "text"
      },
      "source": [
        "\"you\"와 \"i\"의 유사도를 구하는 코드를 구현했다. \n",
        "\n",
        "실행결과 \"you\"와 \"i\"의 유사도는 0.707...로 비교적 유사성이 크다고 볼 수 있다. (코사인 유사도 값은 -1에서 1 사이이므로)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEgiJ8YiWl0B",
        "colab_type": "text"
      },
      "source": [
        "### **2.3.6 유사 단어의 랭킹 표시**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGeUgogrYei5",
        "colab_type": "text"
      },
      "source": [
        "어떤 단어가 검색어로 주어지면, 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수를 구현해보도록 한다. \n",
        "\n",
        "다음은 함수가 입력받을 인수들이다. \n",
        "\n",
        "1. query: 검색어(단어)\n",
        "2. word_to_id: 단어에서 단어 ID로의 딕셔너리\n",
        "3. id_to_word: 단어 ID에서 단어로의 딕셔너리\n",
        "4. word_matrix: 단어 벡터들을 한데 모은 행렬. 각 행에는 대응하는 단어의 벡터가 저장되어 있다고 가정한다. \n",
        "5. top: 상위 몇 개까지 출력할지 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZZ2x35RWSqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "  # 1. 검색어를 꺼낸다.\n",
        "  if query not in word_to_id:\n",
        "    print('%s(을)를 찾을 수 없습니다.'%query)\n",
        "    return\n",
        "\n",
        "  print('\\n[query]'+query)\n",
        "  query_id = word_to_id[query]\n",
        "  query_vec = word_matrix[query_id]\n",
        "\n",
        "  # 2. 코사인 유사도 계산\n",
        "  vocab_size = len(id_to_word)\n",
        "  similarity = np.zeros(vocab_size)\n",
        "  for i in range(vocab_size):\n",
        "    similarity[i] = cos.simiarity(word_matrix[i],query_vec)\n",
        "\n",
        "  # 3. 코사인 유사도를 기준으로 내림차순으로 출력\n",
        "  count = 0\n",
        "  for i in (-1*similarity).argsort():\n",
        "    if id_to_word[i] == query:\n",
        "      continue\n",
        "    print('%s: %s' % (id_to_word[i], similarity[i]))\n",
        "\n",
        "    count += 1\n",
        "    if count >= top:\n",
        "      return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1diNaE8cdXe",
        "colab_type": "text"
      },
      "source": [
        "앞서 설명한 함수를 구현했다.\n",
        "\n",
        "3번 과정의 argsort() 메서드는 넘파이 배열의 원소를 오름차순으로 정렬한다. 단, 반환값은 배열의 인덱스이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs2zp9WTZ6TR",
        "colab_type": "code",
        "outputId": "da21e141-b063-49b9-8e84-6950bef00551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "x = np.array([100,-20,2])\n",
        "x.argsort()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNhltJDxc2Yx",
        "colab_type": "text"
      },
      "source": [
        "argsort() 메서드의 이용 예시이다. argsort() 메서드는 오름차순으로 정렬을 하는데 우리의 목적은 유사도가 큰 순서대로 정렬(즉 내림차순으로 정렬)하는 것이었으므로 넘파이 배열의 각 원소에 마이너스를 곱한 후 이 메서드를 호출하여 원하는 결과를 얻었다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGh2lp2WcxtC",
        "colab_type": "code",
        "outputId": "bc5edd05-0d43-43b6-e426-fb6b32edcd07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "(-x).argsort()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAQAtyX-dNoq",
        "colab_type": "text"
      },
      "source": [
        "앞의 예시에도 적용해 내림차순으로 정렬된 결과를 얻었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhu-P4hXdMOf",
        "colab_type": "code",
        "outputId": "fa374805-d097-4bb2-d9f1-389befae652b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.util import preprocess, create_co_matrix, most_similar\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "\n",
        "most_similar('you', word_to_id, id_to_word, C, top=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[query] you\n",
            " goodbye: 0.7071067691154799\n",
            " i: 0.7071067691154799\n",
            " hello: 0.7071067691154799\n",
            " say: 0.0\n",
            " and: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLN9J-wNd4Cx",
        "colab_type": "text"
      },
      "source": [
        "most_similar() 함수를 사용해 검색어 \"you\"와 유사한 단어 상위 5개를 출력했다. 인칭대명사인 \"i\"와 \"you\"가 유사하다는 것은 납득이 되지만 \"goodbye\", \"hello\"의 코사인 유사도가 높다는 것은 직관적으로 납득이 되지 않는다. 이는 말뭉치가 너무 작아서 발생한 문제이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T56LDdzQp3Sn",
        "colab_type": "text"
      },
      "source": [
        "## **2.4 통계 기반 기법 개선하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kwM7hvNp9yx",
        "colab_type": "text"
      },
      "source": [
        "### **2.4.1 상호정보량**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2004LvsqNzA",
        "colab_type": "text"
      },
      "source": [
        "앞 절에서 본 동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타낸다. 그러나 이 발생 횟수라는 것은 그리 좋은 특성이 아니다. 예를 들어 \"the\"와 \"car\"의 동시발생 횟수는 상당히 많겠지만 관련성이 강하지는 않다. 반면 \"car\"과 \"drive\"는 관련이 깊지만 \"the\"보다 동시발생 횟수가 적어 관련성이 덜 하다는 결과가 나올 것이다. \n",
        "\n",
        "이러한 문제를 해결하기 위해 '점별 상호정보량(PMI)이라는 척도를 사용한다. (PMI의 정의는 교재 p.98 참고)\n",
        "\n",
        "하지만 PMI에도 문제가 있는데, 바로 두 단어의 동시발생 횟수가 0이면 PMI 값이 마이너스 무한대가 된다는 점이다. 이 문제를 피하기 위해 실제로 구현할 때는 양의 상호정보량(PPMI)을 사용한다. (식은 교재 p.99 참고) PPMI 식에 따르면 PMI가 음수일 때는 0으로 취급하게 된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9Ng3ebDdpbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ppmi(C,verbose=False,eps=1e-8):\n",
        "  M = np.zeros_like(C,dtype=np.float32)\n",
        "  N = np.sum(C)\n",
        "  S = np.sum(C, axis=0)\n",
        "  total = C.shape[0]*C.shape[1]\n",
        "  cnt = 0\n",
        "\n",
        "  for i in range(C.shape[0]):\n",
        "    for j in range(C.shape[1]):\n",
        "      pmi = np.log2(C[i,j]*N/(S[j]*S[i])+eps)\n",
        "      M[i,j] = max(0,pmi)\n",
        "\n",
        "      if verbose:\n",
        "        cnt += 1\n",
        "        if cnt%(total//100) == 0:\n",
        "          print('%.1f%% 완료'%(100*cnt/total))\n",
        "  return M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJgD32W2fAAU",
        "colab_type": "text"
      },
      "source": [
        "동시밠애 행렬을 PPMI 행렬로 변환하는 함수를 구현했다. 인수 C는 동시발생 행렬이고, verbose는 진행상황 출력 여부를 결정하는 플래그이다. 큰 말뭉치를 다룰 때 verbose를 True로 설정해주면 중간중간 진행 상황을 알려준다. log2 부분에 eps라는 작은 값을 더해준 것은 np.log2(0)이 음의 무한대가 되는 사태를 피하기 위함이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVwE7UieeWkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "0473ea1b-4941-4ef8-d8b7-5d633ca18724"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.util import preprocess, create_co_matrix, cos_similarity, ppmi\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "W = ppmi(C)\n",
        "\n",
        "np.set_printoptions(precision=3) # 유효 자릿수를 세 자리로 표시\n",
        "print('동시발생 행렬')\n",
        "print(C)\n",
        "print('-'*50)\n",
        "print('PPMI')\n",
        "print(W)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "동시발생 행렬\n",
            "[[0 1 0 0 0 0 0]\n",
            " [1 0 1 0 1 1 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0]]\n",
            "--------------------------------------------------\n",
            "PPMI\n",
            "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
            " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.807 0.    0.    0.    0.    2.807]\n",
            " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH0pGSOJgR_H",
        "colab_type": "text"
      },
      "source": [
        "실제로 동시발생 행렬을 PPMI 행렬로 변환해보았다. \n",
        "\n",
        "동시발생 행렬을 개선하여 PMI 행렬을 얻었고, 이를 또 개선하여 PPMI 행렬을 얻었지만 PPMI 행렬에도 여전히 큰 문제가 있다. 말뭉치의 어휘 수가 증가함에 따라 각 단어 벡터의 차원 수도 증가한다는 문제이다. (벡터의 차원 수는 말뭉치의 어휘 수와 같음) 또한 이 행렬을 들여다보면 원소 대부분이 0인 것을 알 수 있다. 벡터의 원소 대부분이 중요하지 않다는 뜻이다. 다르게 표현하면 각 원소의 중요도가 낮다는 뜻이다. 이런 벡터는 노이즈에 약하고 견고하지 못하다는 약점도 있다. 이 문제에 대처하기 위해 수행하는 기법이 벡터의 차원 감소이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKCukBchhKFJ",
        "colab_type": "text"
      },
      "source": [
        "### **2.4.2 차원 감소**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X08NxaLViHc8",
        "colab_type": "text"
      },
      "source": [
        "차원 감소는 문자 그대로 벡터의 차원을 줄이는 방법을 말한다. 여기서 핵심은 중요한 정보는 최대한 유지하면서 줄이는 것이다. (이를 위해 데이터의 분포를 고려해 중요한 '축'을 찾는 일을 수행한다.) \n",
        "\n",
        "원소 대부분이 0인 행렬(벡터)을 희소행렬(희소벡터)라 한다. 차원 감소의 핵심은 희소벡터에서 중요한 축을 찾아내어 더 적은 차원으로 다시 표현하는 것인데, 차원 감소의 결과로 원래의 희소벡터는 원소 대부분이 0이 아닌 값으로 구성된 밀집벡터로 변환된다. \n",
        "\n",
        "차원을 감소시키는 방법에는 여러 가지가 있는데, 우리는 특잇값분해(SVD)를 이용하기로 한다. SVD에 대한 설명은 교재 p.102~103을 참고한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLlp8p18lUu9",
        "colab_type": "text"
      },
      "source": [
        "### **2.4.3 SVD에 의한 차원 감소**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z3ox_6ygPDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from common.util import preprocess, create_co_matrix, ppmi\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "W = ppmi(C)\n",
        "\n",
        "# SVD\n",
        "U,S,V = np.linalg.svd(W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YivfsCijlwwa",
        "colab_type": "text"
      },
      "source": [
        "동시발생 행렬을 만들어 PPMI 행렬로 변환한 다음 SVD를 적용했다. SVD에 의해 변환된 밀집벡터 표현은 변수 U에 저장했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWIN7b4_lv0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f0a5c5dd-73ae-4d4c-874b-a3ed45790fd6"
      },
      "source": [
        "print(C[0]) # 동시발생 행렬"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA9jjnQCl8pN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "77dcbb5d-76c2-4d70-f4a6-5e0c2027af8e"
      },
      "source": [
        "print(W[0]) # PPMI 행렬"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.    1.807 0.    0.    0.    0.    0.   ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyqaT3o2mApx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d8197c2f-25ba-4740-e241-c181060620e0"
      },
      "source": [
        "print(U[0]) # SVD"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 3.409e-01  0.000e+00 -1.205e-01 -3.886e-16 -9.323e-01 -1.110e-16\n",
            " -2.426e-17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBUr7eTDmQlR",
        "colab_type": "text"
      },
      "source": [
        "단어 ID가 0인 단어 벡터를 확인해보니 원래는 희소벡터인 W[0]이 SVD에 의해서 밀집벡터 U[0]로 변한 것을 알 수 있었다. 밀집벡터의 차원을 감소시키려면, 예컨대 2차원 벡터로 줄이려면 단순히 처음의 두 원소를 꺼내면 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPGY0hU4mG9t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a2f2422a-0f0a-4be9-e2e1-3c13ee8475d4"
      },
      "source": [
        "print(U[0,:2])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.341 0.   ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC7bksqXmktH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1ac05498-b531-4e8b-9966-3b44c4225c97"
      },
      "source": [
        "for word, word_id in word_to_id.items():\n",
        "  plt.annotate(word,(U[word_id,0],U[word_id,1]))\n",
        "\n",
        "plt.scatter(U[:,0],U[:,1],alpha=0.5)\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w+wG7EbthPKGQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD0Z54nNnj3R",
        "colab_type": "text"
      },
      "source": [
        "각 단어를 2차원 벡터로 표현한 후 그래프로 그려보았다. plt.annotate(word,x,y) 메서드는 2차원 그래프상에서 좌표 (x,y) 지점에 word에 담긴 텍스트를 그린다.\n",
        "___\n",
        "goodbye와 hello, you와 i가 제법 가까이 있는 걸로 보아 우리의 직관과 비슷한 결과가 나왔음을 알 수 있었다. 그러나 사용한 말뭉치가 아주 작아서 이 결과를 그대로 받아들이기는 어렵다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifSn9rKioLnw",
        "colab_type": "text"
      },
      "source": [
        "### **2.4.4 PTB 데이터셋**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExvLbdxproly",
        "colab_type": "text"
      },
      "source": [
        "PTB 데이터셋은 적당한 크기의 말뭉치로, 주어진 기법의 품질을 측정하는 벤치마크로 자주 이용된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F78F5MUBniOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "0dd778fd-df31-472d-8fc2-a9e47939b765"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from dataset import ptb\n",
        "\n",
        "corpus,word_to_id,id_to_word = ptb.load_data('train')\n",
        "\n",
        "print('말뭉치 크기: ',len(corpus))\n",
        "print('corpus[:30]: ',corpus[:30])\n",
        "print()\n",
        "print('id_to_word[0]: ',id_to_word[0])\n",
        "print('id_to_word[1]: ',id_to_word[1])\n",
        "print('id_to_word[2]: ',id_to_word[2])\n",
        "print()\n",
        "print(\"word_to_id['car']: \",word_to_id['car'])\n",
        "print(\"word_to_id['happy']: \",word_to_id['happy'])\n",
        "print(\"word_to_id['lexus']: \",word_to_id['lexus'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ptb.train.txt ... \n",
            "Done\n",
            "말뭉치 크기:  929589\n",
            "corpus[:30]:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29]\n",
            "\n",
            "id_to_word[0]:  aer\n",
            "id_to_word[1]:  banknote\n",
            "id_to_word[2]:  berlitz\n",
            "\n",
            "word_to_id['car']:  3856\n",
            "word_to_id['happy']:  4428\n",
            "word_to_id['lexus']:  7426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZYmdNcNuF1T",
        "colab_type": "text"
      },
      "source": [
        "말뭉치를 다루는 방법은 지금까지와 같다. corpus에는 단어 ID 목록이 저장되고, id_to_word는 단어 ID에서 단어로 변환하는 딕셔너리, word_to_id는 단어에서 단어 ID로 변환하는 딕셔너리이다.\n",
        "\n",
        "위의 코드에서 ptb.load_data()는 데이터를 읽어 들인다. 이 때 인수로 'train', 'test', 'valid' 중 하나를 지정할 수 있는데, 차례대로 훈련용, 테스트용, 검증용 데이터를 가리킨다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wp519Nhud5V",
        "colab_type": "text"
      },
      "source": [
        "### **2.4.5 PTB 데이터셋 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pimOiAevt5TL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b5d6e11-9515-4e3b-a5c9-35b9f291bbff"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.util import most_similar, create_co_matrix, ppmi\n",
        "from dataset import ptb\n",
        "\n",
        "window_size = 2\n",
        "wordvec_size = 100\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "print('동시발생 수 계산 ...')\n",
        "C = create_co_matrix(corpus, vocab_size, window_size)\n",
        "print('PPMI 계산 ...')\n",
        "W = ppmi(C, verbose=True)\n",
        "\n",
        "print('SVD 계산 ...')\n",
        "try:\n",
        "  #truncated SVD (빠르다!)\n",
        "  from sklearn.utils.extmath import randomized_svd\n",
        "  U,S,V = randomized_svd(W,n_components=wordvec_size, n_iter=5, random_state=None)\n",
        "\n",
        "except ImportError:\n",
        "  # SVD (느리다)\n",
        "  U,S,V = np.linalg.svd(W)\n",
        "\n",
        "word_vecs = U[:, :wordvec_size]\n",
        "\n",
        "querys = ['you','year','car','toyota']\n",
        "for query in querys:\n",
        "  most_similar(query,word_to_id,id_to_word,word_vecs,top=5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "동시발생 수 계산 ...\n",
            "PPMI 계산 ...\n",
            "1.0% 완료\n",
            "2.0% 완료\n",
            "3.0% 완료\n",
            "4.0% 완료\n",
            "5.0% 완료\n",
            "6.0% 완료\n",
            "7.0% 완료\n",
            "8.0% 완료\n",
            "9.0% 완료\n",
            "10.0% 완료\n",
            "11.0% 완료\n",
            "12.0% 완료\n",
            "13.0% 완료\n",
            "14.0% 완료\n",
            "15.0% 완료\n",
            "16.0% 완료\n",
            "17.0% 완료\n",
            "18.0% 완료\n",
            "19.0% 완료\n",
            "20.0% 완료\n",
            "21.0% 완료\n",
            "22.0% 완료\n",
            "23.0% 완료\n",
            "24.0% 완료\n",
            "25.0% 완료\n",
            "26.0% 완료\n",
            "27.0% 완료\n",
            "28.0% 완료\n",
            "29.0% 완료\n",
            "30.0% 완료\n",
            "31.0% 완료\n",
            "32.0% 완료\n",
            "33.0% 완료\n",
            "34.0% 완료\n",
            "35.0% 완료\n",
            "36.0% 완료\n",
            "37.0% 완료\n",
            "38.0% 완료\n",
            "39.0% 완료\n",
            "40.0% 완료\n",
            "41.0% 완료\n",
            "42.0% 완료\n",
            "43.0% 완료\n",
            "44.0% 완료\n",
            "45.0% 완료\n",
            "46.0% 완료\n",
            "47.0% 완료\n",
            "48.0% 완료\n",
            "49.0% 완료\n",
            "50.0% 완료\n",
            "51.0% 완료\n",
            "52.0% 완료\n",
            "53.0% 완료\n",
            "54.0% 완료\n",
            "55.0% 완료\n",
            "56.0% 완료\n",
            "57.0% 완료\n",
            "58.0% 완료\n",
            "59.0% 완료\n",
            "60.0% 완료\n",
            "61.0% 완료\n",
            "62.0% 완료\n",
            "63.0% 완료\n",
            "64.0% 완료\n",
            "65.0% 완료\n",
            "66.0% 완료\n",
            "67.0% 완료\n",
            "68.0% 완료\n",
            "69.0% 완료\n",
            "70.0% 완료\n",
            "71.0% 완료\n",
            "72.0% 완료\n",
            "73.0% 완료\n",
            "74.0% 완료\n",
            "75.0% 완료\n",
            "76.0% 완료\n",
            "77.0% 완료\n",
            "78.0% 완료\n",
            "79.0% 완료\n",
            "80.0% 완료\n",
            "81.0% 완료\n",
            "82.0% 완료\n",
            "83.0% 완료\n",
            "84.0% 완료\n",
            "85.0% 완료\n",
            "86.0% 완료\n",
            "87.0% 완료\n",
            "88.0% 완료\n",
            "89.0% 완료\n",
            "90.0% 완료\n",
            "91.0% 완료\n",
            "92.0% 완료\n",
            "93.0% 완료\n",
            "94.0% 완료\n",
            "95.0% 완료\n",
            "96.0% 완료\n",
            "97.0% 완료\n",
            "98.0% 완료\n",
            "99.0% 완료\n",
            "100.0% 완료\n",
            "SVD 계산 ...\n",
            "\n",
            "[query] you\n",
            " i: 0.6652265787124634\n",
            " we: 0.6218001246452332\n",
            " do: 0.5773139595985413\n",
            " someone: 0.5663501024246216\n",
            " anybody: 0.5587737560272217\n",
            "\n",
            "[query] year\n",
            " month: 0.7122026085853577\n",
            " quarter: 0.6487215757369995\n",
            " earlier: 0.6392166018486023\n",
            " last: 0.6364238262176514\n",
            " next: 0.5985417366027832\n",
            "\n",
            "[query] car\n",
            " auto: 0.6379319429397583\n",
            " luxury: 0.6050532460212708\n",
            " truck: 0.5071285367012024\n",
            " domestic: 0.5005350708961487\n",
            " corsica: 0.49747809767723083\n",
            "\n",
            "[query] toyota\n",
            " motor: 0.7683124542236328\n",
            " motors: 0.7274988889694214\n",
            " lexus: 0.6681861281394958\n",
            " nissan: 0.658954381942749\n",
            " honda: 0.6247450709342957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msRB7mZH33IK",
        "colab_type": "text"
      },
      "source": [
        "PTB 데이터셋에 통계 기반 기법을 적용해보았다.\n",
        "\n",
        "위 코드에서는 SVD를 수행할 때 sklearn의 randomized_svd() 메서드를 이용했다. 이 메서드는 무작위 수를 사용한 Truncated SVD로, 특잇값이 큰 것들만 계산하여 기본적인 SVD보다 훨씬 빠르다. \n",
        "\n",
        "실행 결과를 보면 우리의 직관과 비슷함을 알 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehJUvLq5xwOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}