{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL2_CH01_김민지.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DDw1s3jNPfm",
        "colab_type": "text"
      },
      "source": [
        "# **Chapter 1 신경망 복습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9saCoqDwNUZm",
        "colab_type": "text"
      },
      "source": [
        "## **1.1 수학과 파이썬 복습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR1EQocSNkNN",
        "colab_type": "text"
      },
      "source": [
        "### **1.1.1 벡터와 행렬**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-Dfk7WCNo4t",
        "colab_type": "text"
      },
      "source": [
        "벡터는 크기와 방향을 가진 양이다. 벡터는 숫자가 일렬로 늘어선 집합으로 표현할 수 있고, 파이썬에서는 1차원 배열로 취급할 수 있다. 그에 반해 행렬은 숫자가 2차원 형태(사각형 형상)로 늘어선 것이다. \n",
        "\n",
        "벡터를 표현하는 방법은 두 가지이다. 하나는 숫자들을 세로로 나열하는 방법(열벡터)이고, 다른 하나는 가로로 나열하는 방법(행벡터)이다. 파이썬으로 구현할 때 벡터를 행벡터로 취급할 경우, 벡터를 가로 방향 행렬로 변환해 사용하면 명확해진다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E7KrpmgNHbA",
        "colab_type": "code",
        "outputId": "4d2fa231-aed0-4f89-a3f2-c25407d34770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x=np.array([1,2,3])\n",
        "x.__class__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFYZg5JrOLqH",
        "colab_type": "text"
      },
      "source": [
        "np.array() 메서드를 이용해 벡터와 행렬을 생성할 수 있다. 이 메서드는 넘파이의 다차원 배열 클래스인 np.ndarray 클래스를 생성한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMkPW3NOOHBw",
        "colab_type": "code",
        "outputId": "f6df50f9-0a7d-4eb9-b1bd-775edd9a8bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "x.shape # 다차원 배열의 형상"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQJ_Vez6OccL",
        "colab_type": "code",
        "outputId": "2154f648-e5ed-4ee0-add9-c094fadd3485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "x.ndim # 차원 수"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS6rIMGPOfRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = np.array([[1,2,3],[4,5,6]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cndg_RclOnk4",
        "colab_type": "code",
        "outputId": "22ab8499-cc31-491b-9302-1f710d6d951b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "W.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLeix9hEOrgk",
        "colab_type": "text"
      },
      "source": [
        "2x3 행렬이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfv1mAc2Ooy4",
        "colab_type": "code",
        "outputId": "a8965b8a-8f86-4459-87d1-0ef586f30f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "W.ndim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eayc3SUzOwY-",
        "colab_type": "text"
      },
      "source": [
        "2차원이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uoYrlohO5CW",
        "colab_type": "text"
      },
      "source": [
        "### **1.1.2 행렬의 원소별 연산**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5gUSrB-OvGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = np.array([[1,2,3],[4,5,6]])\n",
        "X = np.array([[0,1,2],[3,4,5]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVrI2OATPFL7",
        "colab_type": "code",
        "outputId": "108751f1-6028-42d9-8221-232e6642980a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "W+X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  3,  5],\n",
              "       [ 7,  9, 11]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zyKgacxPGDT",
        "colab_type": "code",
        "outputId": "ea0805bc-573f-4da2-a9d8-a1c779515c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "W*X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  2,  6],\n",
              "       [12, 20, 30]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlSKAdjTPMMS",
        "colab_type": "text"
      },
      "source": [
        "피연산자인 다차원 배열들에서 서로 대응하는 원소끼리(각 원소가 독립적으로) 연산이 이루어졌다. 이것이 넘파이 배열의 '원소별 연산'이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCFuvK_xPU0u",
        "colab_type": "text"
      },
      "source": [
        "### **1.1.3 브로드캐스트**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfUwct5APdll",
        "colab_type": "text"
      },
      "source": [
        "넘파이의 다차원 배열에서는 형상이 다른 배열끼리도 연산할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEhrs5arPHvr",
        "colab_type": "code",
        "outputId": "5590772d-6167-4f83-b653-87548b31939b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "A = np.array([[1,2],[3,4]])\n",
        "A*10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10, 20],\n",
              "       [30, 40]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jSxe9WwPjxu",
        "colab_type": "text"
      },
      "source": [
        "2x2 행렬에 스칼라 값을 곱할 수 있다. 스칼라 값 10이 2x2 행렬로 확장되어 원소별 연산이 이루어진다. (다차원 배열의 각 원소에 스칼라 값이 곱해지게 됨)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etuM37DPPbah",
        "colab_type": "code",
        "outputId": "3cbe856e-5417-4227-8266-6e8c7869da7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "A = np.array([[1,2],[3,4]])\n",
        "b = np.array([10,20])\n",
        "A*b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10, 40],\n",
              "       [30, 80]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiQOKAf7QEUl",
        "colab_type": "text"
      },
      "source": [
        "1차원 배열인 b가 A와 형상이 같아지도록 확장되어 계산이 이루어졌다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7_98GUPQLQ8",
        "colab_type": "text"
      },
      "source": [
        "### **1.1.4 벡터의 내적과 행렬의 곱**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQm-1PrsQYG0",
        "colab_type": "text"
      },
      "source": [
        "벡터의 내적은 두 벡터에서 대응하는 원소들의 곱을 모두 더한 것이다.\n",
        "___\n",
        "행렬의 곱은 왼쪽 행렬의 행벡터와 오른쪽 행렬의 열벡터의 내적(원소별 곱의 합)으로 계산한다. 그리고 계산 결과는 새로운 행렬의 대응하는 원소에 저장된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-z9Q9SAQa3U",
        "colab_type": "code",
        "outputId": "68b51db9-21b5-4bc6-aea0-83d4013951b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# 벡터의 내적\n",
        "a = np.array([1,2,3])\n",
        "b = np.array([4,5,6])\n",
        "np.dot(a,b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVH2iVIFRRTj",
        "colab_type": "code",
        "outputId": "35005538-72c5-40cd-ba14-df296bd07fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "np.matmul(a,b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DxUQyd3Q6T1",
        "colab_type": "code",
        "outputId": "201fed0c-f549-4909-9d4d-07dc687b9beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# 행렬의 곱\n",
        "A = np.array([[1,2],[3,4]])\n",
        "B = np.array([[5,6],[7,8]])\n",
        "np.matmul(A,B)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[19, 22],\n",
              "       [43, 50]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egjNhuCJRSpC",
        "colab_type": "code",
        "outputId": "e2caa8e4-93d7-4ae4-df5b-484b1146c895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "np.dot(A,B)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[19, 22],\n",
              "       [43, 50]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwPDxUNNSDoc",
        "colab_type": "text"
      },
      "source": [
        "위의 경우에는 np.dot()과 np.matmul()의 연산 결과가 모두 동일했다. 그러나 np.dot()은 두 배열의 내적곱을 계산해주는 메서드인 반면 np.matmul()은 두 배열의 행렬곱을 계산해준다. 이 둘은 3차원 이상의 행렬을 곱할 때는 서로 다른 결과를 보여주게 되므로 둘을 구분하여 쓰는 것이 낫다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTGeJusMSh_I",
        "colab_type": "text"
      },
      "source": [
        "### **1.1.5 행렬 형상 확인**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBmDXs0CTKel",
        "colab_type": "text"
      },
      "source": [
        "행렬 곱을 할 때는 형상 확인을 통해 대응하는 차원의 원소 수가 같은지를 확인해주어야 한다. 예를 들어 3x2 행렬과 2x4 행렬은 대응하는 차원의 원소 수가 2로 같으므로 행렬 곱이 가능하다. 행렬 곱의 결과로 만들어진 새로운 행렬은 3x4 행렬이 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hghBNA-RFcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = np.array([[1,2],[3,4],[5,6]])\n",
        "B = np.array([[1,2,3,4],[5,6,7,8]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA1QxQv2Tt3s",
        "colab_type": "code",
        "outputId": "ed67982d-7f3f-4392-d0f0-1c43a423f88c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "A.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxIufEn4TxJ-",
        "colab_type": "code",
        "outputId": "e7095ad3-c0fb-4f0a-91b4-1b7b8bd15812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "B.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEOxwmYwTyL2",
        "colab_type": "code",
        "outputId": "fdf4bf64-0355-44be-9b45-54f5ecef8493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "np.matmul(A,B)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[11, 14, 17, 20],\n",
              "       [23, 30, 37, 44],\n",
              "       [35, 46, 57, 68]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFsL0HFzT6ow",
        "colab_type": "text"
      },
      "source": [
        "## **1.2 신경망의 추론**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEBwNtOsUA-7",
        "colab_type": "text"
      },
      "source": [
        "신경망에서 수행하는 작업은 '학습'과 '추론'의 두 단계로 나눌 수 있다. \n",
        "\n",
        "p.30~32 신경망 예시 참고"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Qvg8sOT2Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "W1 = np.random.randn(2,4) # 가중치\n",
        "b1 = np.random.randn(4) # 편향\n",
        "x = np.random.randn(10,2) # 입력\n",
        "h = np.matmul(x,W1) + b1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIpCToGOVL7l",
        "colab_type": "text"
      },
      "source": [
        "완전연결계층에 의한 변환의 미니배치 버전을 구현했다.\n",
        "___\n",
        "그런데 완전연결계층에 의한 변환은 '선형 변환'이다. 여기에 비선형 효과를 부여하는 것이 바로 활성화 함수이다. 비선형 활성화 함수를 이용해 신경망의 표현력을 높일 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7u2i0qvVKu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Qw8sg-XCVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = sigmoid(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_sNt63vXFXK",
        "colab_type": "text"
      },
      "source": [
        "시그모이드 함수를 사용하여 은닉층 뉴런을 변환하였다. 계속해서 이 활성화 함수의 출력인 a를 또 다른 완전연결계층에 통과시켜 변환한다. 이것으로 출력층의 뉴런을 얻을 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXy_7NXIXDxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "x = np.random.randn(10,2) # 입력\n",
        "W1 = np.random.randn(2,4) # 가중치1\n",
        "b1 = np.random.randn(4) # 편향1\n",
        "W2 = np.random.randn(4,3) # 가중치2\n",
        "b2 = np.random.randn(3) # 편향2\n",
        "\n",
        "h = np.matmul(x,W1) + b1 # 입력x가중치1 + 편향1\n",
        "a = sigmoid(h) # 활성화\n",
        "s = np.matmul(a,W2) + b2 # 입력x가중치2 + 편향2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYUoW_DUYCdW",
        "colab_type": "text"
      },
      "source": [
        "예시로 든 신경망을 종합적으로 구현했다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBUmL3rEYI-X",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.2 계층으로 클래스화 및 순전파 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj_eMmvNbny5",
        "colab_type": "text"
      },
      "source": [
        "신경망에서 하는 처리를 계층으로 구현해본다. 완전연결계층에 의한 변환을 Affine 계층으로, 시그모이드 함수에 의한 변환을 Sigmoid 계층으로 구현한다. \n",
        "___\n",
        "계층 구현 시 따라야 하는 규칙 두 가지:\n",
        " 1. 모든 계층은 forward()와 backward() 메서드를 가진다.\n",
        " 2. 모든 계층은 인스턴스 변수인 params와 grads를 가진다. \n",
        "___\n",
        "forward()와 backward() 메서드는 각각 순전파와 역전파를 수행한다. 입력층에서 출력층으로 처리 결과를 차례로 전파해가는 것이 순전파, 기울기를 순전파와는 반대 방향으로 전파하는 것이 역전파이다. \n",
        "\n",
        "params는 가중치와 편향 같은 매개변수를 담는 리스트이다. 매개변수는 여러 개가 있을 수 있어서 리스트에 보관한다. grads는 params에 저장된 각 매개변수에 대응하여, 해당 매개변수의 기울기를 보관하는 리스트이다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpg4nkHdYBgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.params=[] # 학습하는 매개변수가 따로 없으므로 params는 빈 리스트로 초기화했다.\n",
        "\n",
        "  def forward(self,x):\n",
        "    return 1 / (1+np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogky2XnhbjUP",
        "colab_type": "text"
      },
      "source": [
        "시그모이드 계층을 구현했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j19NPI6zbU-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Affine: \n",
        "  def __init__(self,W,b):\n",
        "    self.params = [W,b]\n",
        "\n",
        "  def forward(self,x):\n",
        "    W,b = self.params\n",
        "    out = np.matmul(x,W) + b\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29oL6ZGCc7nM",
        "colab_type": "text"
      },
      "source": [
        "Affine 계층을 구현했다. Affine 계층은 초기화될 때 가중치와 편향을 받는다. 즉 가중치와 편향은 Affine 계층의 배개변수이며 이 두 매개변수는 신경망이 학습될 때 수시로 갱신된다. \n",
        "\n",
        "forward(x)는 순전파 처리를 구현한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZeropibc6aN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    I,H,O = input_size, hidden_size, output_size\n",
        "\n",
        "    # 가중치와 편향 초기화\n",
        "    W1 = np.random.randn(I,H)\n",
        "    b1 = np.random.randn(H)\n",
        "    W2 = np.random.randn(H,O)\n",
        "    b2 = np.random.randn(O)\n",
        "\n",
        "    # 계층 생성\n",
        "    self.layers = [\n",
        "       Affine(W1,b1),\n",
        "       Sigmoid(),\n",
        "       Affine(W2,b2)\n",
        "    ]\n",
        "\n",
        "    #모든 가중치를 리스트에 모은다.\n",
        "    self.params = []\n",
        "    for layer in self.layers:\n",
        "      self.params += layer.params\n",
        "    \n",
        "  def predict(self,x):\n",
        "    for layer in self.layers:\n",
        "      x=layer.forward(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzMZpXbNevUp",
        "colab_type": "text"
      },
      "source": [
        "최과 메서드로 먼저 가중치를 초기화하고 3개의 계층을 생성했다. 마지막으로는 학습해야 할 가중치 매개변수들을 params 리스트에 저장했다. 매개변수들을 하나의 리스트에 보관하면 매개변수 갱신과 매개변수 저장을 손쉽게 처리할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ursiuZmSeoCx",
        "colab_type": "code",
        "outputId": "f637577b-8081-489d-9779-5956b9ccc803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a = ['A','B']\n",
        "a += ['C','D']\n",
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'B', 'C', 'D']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llTrLRZ8fLRb",
        "colab_type": "text"
      },
      "source": [
        "이처럼 + 연산자는 리스트들을 결합해준다. TwoLayerNet의 구현에서 각 계층의 params 리스트를 더해주어 모든 학습 매개변수를 하나의 리스트에 담은 것과 같은 원리다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrMtDHnpfKd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.random.randn(10,2)\n",
        "model = TwoLayerNet(2,4,3)\n",
        "s = model.predict(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLtazQLsff_S",
        "colab_type": "code",
        "outputId": "fe45b660-631c-430e-bd36-8a5d4dda299d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.92012778,  0.09157693, -3.34184714],\n",
              "       [-0.12553199,  0.09792607, -3.39008552],\n",
              "       [ 1.25165076,  0.26119198, -3.35554848],\n",
              "       [ 1.15171427,  0.07852073, -3.32074562],\n",
              "       [ 0.62669916, -0.32185263, -3.53445834],\n",
              "       [ 0.67173164,  0.13898902, -3.37823353],\n",
              "       [ 0.92886492, -0.01637214, -3.32040586],\n",
              "       [ 1.29054898,  0.65847687, -3.47678272],\n",
              "       [ 1.25925369,  0.30943014, -3.37557145],\n",
              "       [ 0.77319994,  0.28154284, -3.40667252]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzFCgabafmyX",
        "colab_type": "text"
      },
      "source": [
        "신경망의 추론을 수행했다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v-RtXczhJI1",
        "colab_type": "text"
      },
      "source": [
        "## **1.3 신경망의 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjDQS0TmV1_n",
        "colab_type": "text"
      },
      "source": [
        "1.3.1부터 1.3.4의 '분기 노드'까지(p.39~49)는 교재 참고"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibms_AY1YwPo",
        "colab_type": "text"
      },
      "source": [
        "### **1.3.4 계산 그래프**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPJ3849udz1e",
        "colab_type": "text"
      },
      "source": [
        "**Repeat 노드**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTxu3f7ZYz5X",
        "colab_type": "text"
      },
      "source": [
        "Repeat 노드는 분기 노드를 일반화한 것으로, 2개가 아닌 N개로 분기(복제)되는 노드를 말한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LVp9TMmfl_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "D,N = 8,7\n",
        "x = np.random.randn(1,D) # 입력\n",
        "y = np.repeat(x,N,axis=0) # 순전파\n",
        "dy = np.random.randn(N,D) # 무작위 기울기\n",
        "dx = np.sum(dy,axis=0,keepdims=True) # 역전파 # 역전파될 때 기울기를 모두 더하는 것을 구현"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WzhMF6WaP7R",
        "colab_type": "code",
        "outputId": "1657719b-3e57-4b49-f021-43be6549ceb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.02437981, -1.14604593,  0.36461882, -1.84305614,  0.63622896,\n",
              "        -0.09799527, -0.41492896,  1.38913312],\n",
              "       [-1.02437981, -1.14604593,  0.36461882, -1.84305614,  0.63622896,\n",
              "        -0.09799527, -0.41492896,  1.38913312],\n",
              "       [-1.02437981, -1.14604593,  0.36461882, -1.84305614,  0.63622896,\n",
              "        -0.09799527, -0.41492896,  1.38913312],\n",
              "       [-1.02437981, -1.14604593,  0.36461882, -1.84305614,  0.63622896,\n",
              "        -0.09799527, -0.41492896,  1.38913312],\n",
              "       [-1.02437981, -1.14604593,  0.36461882, -1.84305614,  0.63622896,\n",
              "        -0.09799527, -0.41492896,  1.38913312],\n",
              "       [-1.02437981, -1.14604593,  0.36461882, -1.84305614,  0.63622896,\n",
              "        -0.09799527, -0.41492896,  1.38913312],\n",
              "       [-1.02437981, -1.14604593,  0.36461882, -1.84305614,  0.63622896,\n",
              "        -0.09799527, -0.41492896,  1.38913312]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iukq4sDRaDwL",
        "colab_type": "text"
      },
      "source": [
        "길이가 8인 배열을 7개로 복제하는 Repeat 노드를 구현하였다. np.repeat() 메서드가 원소 복제를 수행해준다. 여기서 axis를 지정하여 어느 축 방향으로 복제할지를 조정할 수 있다. 역전파에서는 총합을 구해야 해서 np.sum() 메서드를 이용했고, 여기서도 axis 인수를 설정하여 어느 축 방향으로 합을 구할지 지정한다. 또한 인수로 keepdims=True를 설정하여 2차원 배열의 차원 수를 유지한다. 이 예에서는 keepdims가 True여서 (1,D)로 형상이 유지되지만, False로 지정하면 (D)가 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaoy9ZMnd3dG",
        "colab_type": "text"
      },
      "source": [
        "**Sum 노드**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__MdI9aAbS4T",
        "colab_type": "text"
      },
      "source": [
        "Sum 노드는 범용 덧셈 노드이다. Sum 노드의 역전파는 상류로부터의 기울기를 모든 화살표에 분배한다. 덧셈 노드의 역전파를 자연스럽게 확장한 것이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12WZbYTjZeVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "D,N = 8,7\n",
        "x = np.random.randn(N,D) # 입력\n",
        "y = np.sum(x,axis=0,keepdims=True) # 순전파\n",
        "\n",
        "dy = np.random.randn(1,D) # 무작위 기울기\n",
        "dx = np.repeat(dy,N,axis=0) # 역전파"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygLbkjosb1g9",
        "colab_type": "text"
      },
      "source": [
        "Sum 노드를 구현했다. 순전파에서는 np.sum() 메서드를 이용해주었고, 역전파는 np.repeat() 메서드로 구현했다. 여기서 체크할 부분은 Sum 노드와 Repeat 노드가 서로 반대 관계라는 것이다. Sum 노드의 순전파가 Repeat 노드의 역전파가 되며, Sum 노드의 역전파가 Repeat 노드의 순전파가 된다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYxJotcAd5t3",
        "colab_type": "text"
      },
      "source": [
        "**MatMul 노드**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giSpnw5seqGe",
        "colab_type": "text"
      },
      "source": [
        "MatMul 노드에 관한 설명은 교재 p.51~54 참고"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-IXnhUrb0EC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MatMul:\n",
        "  def __init__(self,W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.x = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    W, = self.params\n",
        "    out = np.matmul(x,W)\n",
        "    self.x = x\n",
        "    return out\n",
        "\n",
        "  def backward(self,dout):\n",
        "    W, = self.params\n",
        "    dx = np.matmul(dout,W.T)\n",
        "    dW = np.matmul(self.x.T,dout)\n",
        "    self.grads[0][...] = dW\n",
        "    return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxtX1P3VfX8-",
        "colab_type": "text"
      },
      "source": [
        "MatMul 노드를 구현했다. MatMul 계층은 학습하는 매개변수를 params에 보관한다. 그리고 거기에 대응시키는 형태로, 기울기는 grads에 보관한다. 역전파에서는 dx와 dW를 구해 가중치의 기울기를 인스턴스 변수인 grads에 저장했다. \n",
        "\n",
        "참고로, 기울기 값을 설정하는 self.grads[0][...] = dW 코드에서 점 3개로 이루어진 생략 기호를 사용했다. 이렇게 하면 넘파이 배열이 가리키는 메모리 위치를 고정시킨 다음, 그 위치에 원소들을 덮어쓴다. grads[0] = dW 처럼 할당하게 되면 얕은 복사가 이루어지게 되고, 생략 기호를 이용해서 덮어쓰면 깊은 복사가 이루어진다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrnWBwxmfWts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.array([1,2,3])\n",
        "b = np.array([4,5,6])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDXtUNW3gdpw",
        "colab_type": "text"
      },
      "source": [
        "교재 p.55의 설명을 참고한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE6V0FAVikaz",
        "colab_type": "text"
      },
      "source": [
        "### **1.3.5 기울기 도출과 역전파 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68JjpOIdisI2",
        "colab_type": "text"
      },
      "source": [
        "**Sigmoid 계층**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-uIfH2jjYWw",
        "colab_type": "text"
      },
      "source": [
        "설명은 교재 p.56 참고"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhD84267gHdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.params,self.grads=[],[]\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = 1 / (1+np.exp(-x)) # 출력을 인스턴스 변수 out에 저장했다. \n",
        "    self.out = out \n",
        "    return out\n",
        "\n",
        "  def backward(self,dout):\n",
        "    dx = dout*(1.0-self.out)*self.out # self.out에 저장해둔 out 값을 이용했다. \n",
        "    return dx "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1-7P5Ncj_LU",
        "colab_type": "text"
      },
      "source": [
        "Sigmoid 계층을 구현했다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e8yM4JbkCxt",
        "colab_type": "text"
      },
      "source": [
        "**Affine 계층**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCXtm4kKkKz5",
        "colab_type": "text"
      },
      "source": [
        "Affine 계층의 순전파는 y = np.matmul(x,W) + b로 구현할 수 있다. 여기서 편향을 더할 때는 넘파이의 브로드캐스트가 사용된다. (계산 그래프로 그렸을 때, Repeat 노드가 수행하는 복제가 넘파이의 브로드캐스트 기능에 해당함 / 계산 그래프는 교재 p.58 참고)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmLe0F7Aj-Jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Affine:\n",
        "  def __init__(slef,W,b):\n",
        "    self.params = [W,b]\n",
        "    self.grads = [np.zeros_like(W),np.zeros_like(b)]\n",
        "    self.x = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    W,b = self.params\n",
        "    out = np.matmul(x,W) + b\n",
        "    self.x = x\n",
        "    return out\n",
        "\n",
        "  def backward(self,dout):\n",
        "    W,b = self.params\n",
        "    dx = np.matmul(dout,W.T)\n",
        "    dW = np.matmul(self.x.T,dout)\n",
        "    db = np.sum(dout, axis=0)\n",
        "\n",
        "    self.grads[0][...] = dW\n",
        "    self.grads[1][...] = db\n",
        "    return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVxg0fINmu6R",
        "colab_type": "text"
      },
      "source": [
        "Affine 계층을 구현했다. 인스턴스 변수 params에는 매개변수를, grads에는 기울기를 저장했다. Affine의 역전파는 MatMul 노드와 Repeat 노드의 역전파를 수행하면 구할 수 있다. Repeat 노드의 역전파는 np.sum() 메서드로 계산할 수 있는데, 이 때 행렬의 형상을 잘 살펴보고 어느 축(axis)으로 합을 구할지를 명시해야 한다. 마지막으로, 가중치 매개변수의 기울기를 인스턴스 변수 grads에 저장한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfoz__ryncL4",
        "colab_type": "text"
      },
      "source": [
        "**Softmax with Loss 계층**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLTY-nednixZ",
        "colab_type": "text"
      },
      "source": [
        "소프트맥스 함수와 교차 엔트로피 오차는 Softmax with Loss 라는 하나의 계층으로 구현할 것이다. 소프트맥스 계층은 입력을 받아 정규화하여 출력해주고, Cross Entropy Error 계층은 Softmax의 출력과 정답 레이블을 받아 이 데이터로부터 손실 L을 구해 출력해준다.\n",
        "\n",
        "계산 그래프는 교재 p.59 참고"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgxyIv5moHy9",
        "colab_type": "text"
      },
      "source": [
        "### **1.3.6 가중치 갱신**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGO4yEL4oLRe",
        "colab_type": "text"
      },
      "source": [
        "오차 역전파법으로 기울기를 구했으면, 그 기울기를 사용해 신경망의 매개변수를 갱신한다. 이 때 신경망의 학습은 다음 순서로 수행한다.\n",
        "___\n",
        " 1. 미니배치: 훈련 데이터 중에서 무작위로 다수의 데이터를 골라낸다.\n",
        " 2. 기울기 계산: 오차역전파법으로 각 가중치 매개변수에 대한 손실 함수의 기울기를 구한다.\n",
        " 3. 매개변수 갱신: 기울기를 사용하여 가중치 매개변수를 갱신한다. 오차역전파법으로 얻은 기울기는 손실을 가장 크게 하는 방향을 가리키므로 매개변수를 그 기울기와 반대 방향으로 갱신하면 손실을 줄일 수 있다. 이것을 경사하강법이라고 한다.\n",
        " 4. 1~3단계를 필요한 만큼 반복한다.\n",
        " ___\n",
        " 3단계에서 수행하는 가중치 갱신 기법의 종류는 아주 다양한데, 가장 단순한 것은 확률적경사하강법(SGD)이다. 참고로, '확률적'은 무작위로 선택된 데이터(미니배치)에 대한 기울기를 이용한다는 뜻이다. SGD는 가중치를 기울기 방향으로 일정한 거리만큼 갱신한다. \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQrJi3wnms0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SGD:\n",
        "  def __init__(self,lr=0.01):\n",
        "    self.lr = lr\n",
        "\n",
        "  def update(self,params,grads):\n",
        "    for i in range(len(params)):\n",
        "      params[i] -= self.lr*grad[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzBYwBY5qu6_",
        "colab_type": "text"
      },
      "source": [
        "SGD를 구현했다. 초기화 인수 lr은 학습률을 뜻하고, 그 값을 인스턴스 변수로 저장해둔다. 그리고 update(params,grads) 메서드는 매개변수 갱신을 처리한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2XBzofNrEzx",
        "colab_type": "text"
      },
      "source": [
        "## **1.4 신경망으로 문제를 풀다**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adGIvpVurzBR",
        "colab_type": "text"
      },
      "source": [
        "### **1.4.1 스파이럴 데이터셋**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8AvoxnMqg1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..') # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import spiral\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x,t = spiral.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqIiX87As8_0",
        "colab_type": "code",
        "outputId": "d1b1f75b-64eb-4bbf-8ece-ec59d0b1b032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print('x',x.shape) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x (300, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q44eBMaOtMFT",
        "colab_type": "text"
      },
      "source": [
        "300개의 샘플 데이터를 담고 있는 2차원 데이터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IvuGmqXtBow",
        "colab_type": "code",
        "outputId": "e1a9ca9e-335b-42ae-c8a1-fa8212b0644e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print('t',t.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t (300, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNpfqJdytPL0",
        "colab_type": "text"
      },
      "source": [
        "300개의 샘플 데이터를 담고 있는 3차원 데이터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phisLcL5tbht",
        "colab_type": "text"
      },
      "source": [
        "### **1.4.2 신경망 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LCqDr0yuW9B",
        "colab_type": "text"
      },
      "source": [
        "은닉층이 하나인 신경망을 구현한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzCCprOrtFWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.layers import Affine, Sigmoid, SoftmaxWithLoss\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    I,H,O = input_size, hidden_size, output_size\n",
        "\n",
        "    # 가중치와 편향 초기화\n",
        "    W1 = 0.01 * np.random.randn(I,H)\n",
        "    b1 = np.zeros(H)\n",
        "    W2 = 0.01 * np.random.randn(H,O)\n",
        "    b2 = np.zeros(O)\n",
        "\n",
        "    # 계층 생성\n",
        "    self.layers = [\n",
        "       Affine(W1, b1),\n",
        "       Sigmoid(),\n",
        "       Affine(W2, b2)\n",
        "    ]\n",
        "    self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "    # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in self.layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "    \n",
        "  def predict(self,x):\n",
        "    for layer in self.layers:\n",
        "      x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self,x,t):\n",
        "    score = self.predict(x)\n",
        "    loss = self.loss_layer.forward(score,t)\n",
        "    return loss\n",
        "\n",
        "  def backward(self,dout=1):\n",
        "    dout = self.loss_layer.backward(dout)\n",
        "    for layer in reversed(self.layers):\n",
        "      dout = layer.backward(dout)\n",
        "    return dout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzNhXWazxrwL",
        "colab_type": "text"
      },
      "source": [
        "초기화 메서드는 3개의 인수를 받는데, 차례대로 입력층의 뉴런 수, 은닉층의 뉴런 수, 출력층의 뉴런 수를 말한다. 메서드 안에서 우선 편향을 영벡터로 초기화하고(np.zeros() 사용), 가중치는 작은 무작위 값으로 초기화 한다. 가중치를 작은 무작위 값으로 설정하면 학습이 잘 진행될 가능성이 커진다. 계속해서 필요한 계층을 생성해 인스턴스 변수 layers에 모아두고, 마지막으로 이 모델에서 사용하는 매개변수들과 기울기들을 각각 하나로 모은다.\n",
        "\n",
        "이어서 TwoLayerNet에 3개의 메서드를 구현해 넣는다. 추론을 수행하는 predict 메서드, 순전파를 담당하는 forward 메서드, 역전파를 담당하는 backward 메서드이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dte8EdbLzQwO",
        "colab_type": "text"
      },
      "source": [
        "### **1.4.3 학습용 코드**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpFGuLr6vdca",
        "colab_type": "code",
        "outputId": "516fcdbc-8a58-406c-f990-3346772504d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.optimizer import SGD\n",
        "from dataset import spiral\n",
        "import matplotlib.pyplot as plt\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 1. 하이퍼파라미터 설정\n",
        "max_epoch = 300\n",
        "batch_size = 30\n",
        "hidden_size = 10\n",
        "learning_rate = 1.0\n",
        "\n",
        "# 2. 데이터 읽기, 모델과 옵티마이저 생성\n",
        "x,t = spiral.load_data()\n",
        "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
        "optimizer = SGD(lr=learning_rate)\n",
        "\n",
        "# 학습에 사용하는 변수\n",
        "data_size = len(x)\n",
        "max_iters = data_size//batch_size\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "loss_list = []\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "  # 3. 데이터 뒤섞기 \n",
        "  idx = np.random.permutation(data_size)\n",
        "  x = x[idx]\n",
        "  t = t[idx]\n",
        "\n",
        "  for iters in range(max_iters):\n",
        "    batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "    batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "  # 4. 기울기를 구해 매개변수 갱신 \n",
        "  loss = model.forward(batch_x, batch_t)\n",
        "  model.backward()\n",
        "  optimizer.update(model.params,model.grads)\n",
        "\n",
        "  total_loss += loss\n",
        "  loss_count += 1\n",
        "\n",
        "  # 5. 정기적으로 학습 경과 출력\n",
        "  if (iters+1)%10 == 0:\n",
        "    avg_loss = total_loss / loss_count\n",
        "    print('|에폭%d| 반복%d/%d | 손실%.2f'\n",
        "          %(epoch+1,iters+1,max_iters,avg_loss))\n",
        "    loss_list.append(avg_loss)\n",
        "    total_loss, loss_count = 0,0\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|에폭1| 반복10/10 | 손실1.10\n",
            "|에폭2| 반복10/10 | 손실1.12\n",
            "|에폭3| 반복10/10 | 손실1.10\n",
            "|에폭4| 반복10/10 | 손실1.08\n",
            "|에폭5| 반복10/10 | 손실1.18\n",
            "|에폭6| 반복10/10 | 손실1.22\n",
            "|에폭7| 반복10/10 | 손실1.24\n",
            "|에폭8| 반복10/10 | 손실1.13\n",
            "|에폭9| 반복10/10 | 손실1.12\n",
            "|에폭10| 반복10/10 | 손실1.20\n",
            "|에폭11| 반복10/10 | 손실1.22\n",
            "|에폭12| 반복10/10 | 손실1.22\n",
            "|에폭13| 반복10/10 | 손실1.10\n",
            "|에폭14| 반복10/10 | 손실1.12\n",
            "|에폭15| 반복10/10 | 손실1.24\n",
            "|에폭16| 반복10/10 | 손실1.32\n",
            "|에폭17| 반복10/10 | 손실1.16\n",
            "|에폭18| 반복10/10 | 손실1.10\n",
            "|에폭19| 반복10/10 | 손실1.13\n",
            "|에폭20| 반복10/10 | 손실1.12\n",
            "|에폭21| 반복10/10 | 손실1.06\n",
            "|에폭22| 반복10/10 | 손실1.42\n",
            "|에폭23| 반복10/10 | 손실1.11\n",
            "|에폭24| 반복10/10 | 손실1.17\n",
            "|에폭25| 반복10/10 | 손실1.06\n",
            "|에폭26| 반복10/10 | 손실1.24\n",
            "|에폭27| 반복10/10 | 손실1.11\n",
            "|에폭28| 반복10/10 | 손실1.11\n",
            "|에폭29| 반복10/10 | 손실1.11\n",
            "|에폭30| 반복10/10 | 손실1.13\n",
            "|에폭31| 반복10/10 | 손실1.13\n",
            "|에폭32| 반복10/10 | 손실1.24\n",
            "|에폭33| 반복10/10 | 손실1.20\n",
            "|에폭34| 반복10/10 | 손실1.08\n",
            "|에폭35| 반복10/10 | 손실1.15\n",
            "|에폭36| 반복10/10 | 손실1.10\n",
            "|에폭37| 반복10/10 | 손실1.14\n",
            "|에폭38| 반복10/10 | 손실1.13\n",
            "|에폭39| 반복10/10 | 손실1.15\n",
            "|에폭40| 반복10/10 | 손실1.34\n",
            "|에폭41| 반복10/10 | 손실1.16\n",
            "|에폭42| 반복10/10 | 손실1.13\n",
            "|에폭43| 반복10/10 | 손실1.20\n",
            "|에폭44| 반복10/10 | 손실1.12\n",
            "|에폭45| 반복10/10 | 손실1.10\n",
            "|에폭46| 반복10/10 | 손실1.10\n",
            "|에폭47| 반복10/10 | 손실1.10\n",
            "|에폭48| 반복10/10 | 손실1.20\n",
            "|에폭49| 반복10/10 | 손실1.29\n",
            "|에폭50| 반복10/10 | 손실1.18\n",
            "|에폭51| 반복10/10 | 손실1.12\n",
            "|에폭52| 반복10/10 | 손실1.14\n",
            "|에폭53| 반복10/10 | 손실1.12\n",
            "|에폭54| 반복10/10 | 손실1.17\n",
            "|에폭55| 반복10/10 | 손실1.23\n",
            "|에폭56| 반복10/10 | 손실1.12\n",
            "|에폭57| 반복10/10 | 손실1.10\n",
            "|에폭58| 반복10/10 | 손실1.17\n",
            "|에폭59| 반복10/10 | 손실1.26\n",
            "|에폭60| 반복10/10 | 손실1.09\n",
            "|에폭61| 반복10/10 | 손실1.14\n",
            "|에폭62| 반복10/10 | 손실1.16\n",
            "|에폭63| 반복10/10 | 손실1.10\n",
            "|에폭64| 반복10/10 | 손실1.12\n",
            "|에폭65| 반복10/10 | 손실1.08\n",
            "|에폭66| 반복10/10 | 손실1.15\n",
            "|에폭67| 반복10/10 | 손실1.07\n",
            "|에폭68| 반복10/10 | 손실1.17\n",
            "|에폭69| 반복10/10 | 손실1.09\n",
            "|에폭70| 반복10/10 | 손실1.18\n",
            "|에폭71| 반복10/10 | 손실1.10\n",
            "|에폭72| 반복10/10 | 손실1.12\n",
            "|에폭73| 반복10/10 | 손실1.11\n",
            "|에폭74| 반복10/10 | 손실1.17\n",
            "|에폭75| 반복10/10 | 손실1.17\n",
            "|에폭76| 반복10/10 | 손실1.22\n",
            "|에폭77| 반복10/10 | 손실1.22\n",
            "|에폭78| 반복10/10 | 손실1.06\n",
            "|에폭79| 반복10/10 | 손실1.10\n",
            "|에폭80| 반복10/10 | 손실1.12\n",
            "|에폭81| 반복10/10 | 손실1.21\n",
            "|에폭82| 반복10/10 | 손실1.15\n",
            "|에폭83| 반복10/10 | 손실1.07\n",
            "|에폭84| 반복10/10 | 손실1.15\n",
            "|에폭85| 반복10/10 | 손실1.08\n",
            "|에폭86| 반복10/10 | 손실1.10\n",
            "|에폭87| 반복10/10 | 손실1.12\n",
            "|에폭88| 반복10/10 | 손실1.10\n",
            "|에폭89| 반복10/10 | 손실1.08\n",
            "|에폭90| 반복10/10 | 손실1.14\n",
            "|에폭91| 반복10/10 | 손실1.08\n",
            "|에폭92| 반복10/10 | 손실1.11\n",
            "|에폭93| 반복10/10 | 손실1.10\n",
            "|에폭94| 반복10/10 | 손실1.11\n",
            "|에폭95| 반복10/10 | 손실1.15\n",
            "|에폭96| 반복10/10 | 손실1.13\n",
            "|에폭97| 반복10/10 | 손실1.10\n",
            "|에폭98| 반복10/10 | 손실1.07\n",
            "|에폭99| 반복10/10 | 손실1.17\n",
            "|에폭100| 반복10/10 | 손실1.21\n",
            "|에폭101| 반복10/10 | 손실1.15\n",
            "|에폭102| 반복10/10 | 손실1.11\n",
            "|에폭103| 반복10/10 | 손실1.10\n",
            "|에폭104| 반복10/10 | 손실1.10\n",
            "|에폭105| 반복10/10 | 손실1.18\n",
            "|에폭106| 반복10/10 | 손실1.09\n",
            "|에폭107| 반복10/10 | 손실1.11\n",
            "|에폭108| 반복10/10 | 손실1.15\n",
            "|에폭109| 반복10/10 | 손실1.28\n",
            "|에폭110| 반복10/10 | 손실1.14\n",
            "|에폭111| 반복10/10 | 손실1.10\n",
            "|에폭112| 반복10/10 | 손실1.10\n",
            "|에폭113| 반복10/10 | 손실1.10\n",
            "|에폭114| 반복10/10 | 손실1.08\n",
            "|에폭115| 반복10/10 | 손실1.08\n",
            "|에폭116| 반복10/10 | 손실1.13\n",
            "|에폭117| 반복10/10 | 손실1.12\n",
            "|에폭118| 반복10/10 | 손실1.08\n",
            "|에폭119| 반복10/10 | 손실1.09\n",
            "|에폭120| 반복10/10 | 손실1.11\n",
            "|에폭121| 반복10/10 | 손실1.06\n",
            "|에폭122| 반복10/10 | 손실1.13\n",
            "|에폭123| 반복10/10 | 손실1.12\n",
            "|에폭124| 반복10/10 | 손실1.05\n",
            "|에폭125| 반복10/10 | 손실1.05\n",
            "|에폭126| 반복10/10 | 손실1.10\n",
            "|에폭127| 반복10/10 | 손실1.06\n",
            "|에폭128| 반복10/10 | 손실1.06\n",
            "|에폭129| 반복10/10 | 손실1.06\n",
            "|에폭130| 반복10/10 | 손실1.07\n",
            "|에폭131| 반복10/10 | 손실1.05\n",
            "|에폭132| 반복10/10 | 손실1.04\n",
            "|에폭133| 반복10/10 | 손실1.10\n",
            "|에폭134| 반복10/10 | 손실1.04\n",
            "|에폭135| 반복10/10 | 손실1.02\n",
            "|에폭136| 반복10/10 | 손실1.09\n",
            "|에폭137| 반복10/10 | 손실1.08\n",
            "|에폭138| 반복10/10 | 손실0.98\n",
            "|에폭139| 반복10/10 | 손실1.18\n",
            "|에폭140| 반복10/10 | 손실0.98\n",
            "|에폭141| 반복10/10 | 손실1.03\n",
            "|에폭142| 반복10/10 | 손실1.05\n",
            "|에폭143| 반복10/10 | 손실1.04\n",
            "|에폭144| 반복10/10 | 손실1.06\n",
            "|에폭145| 반복10/10 | 손실1.01\n",
            "|에폭146| 반복10/10 | 손실1.07\n",
            "|에폭147| 반복10/10 | 손실1.04\n",
            "|에폭148| 반복10/10 | 손실1.01\n",
            "|에폭149| 반복10/10 | 손실1.00\n",
            "|에폭150| 반복10/10 | 손실1.02\n",
            "|에폭151| 반복10/10 | 손실1.04\n",
            "|에폭152| 반복10/10 | 손실0.96\n",
            "|에폭153| 반복10/10 | 손실1.01\n",
            "|에폭154| 반복10/10 | 손실1.00\n",
            "|에폭155| 반복10/10 | 손실1.01\n",
            "|에폭156| 반복10/10 | 손실0.98\n",
            "|에폭157| 반복10/10 | 손실1.11\n",
            "|에폭158| 반복10/10 | 손실1.04\n",
            "|에폭159| 반복10/10 | 손실0.95\n",
            "|에폭160| 반복10/10 | 손실0.93\n",
            "|에폭161| 반복10/10 | 손실1.03\n",
            "|에폭162| 반복10/10 | 손실0.97\n",
            "|에폭163| 반복10/10 | 손실1.07\n",
            "|에폭164| 반복10/10 | 손실0.96\n",
            "|에폭165| 반복10/10 | 손실0.91\n",
            "|에폭166| 반복10/10 | 손실0.95\n",
            "|에폭167| 반복10/10 | 손실0.96\n",
            "|에폭168| 반복10/10 | 손실0.91\n",
            "|에폭169| 반복10/10 | 손실0.95\n",
            "|에폭170| 반복10/10 | 손실0.99\n",
            "|에폭171| 반복10/10 | 손실0.87\n",
            "|에폭172| 반복10/10 | 손실1.01\n",
            "|에폭173| 반복10/10 | 손실0.83\n",
            "|에폭174| 반복10/10 | 손실1.05\n",
            "|에폭175| 반복10/10 | 손실0.89\n",
            "|에폭176| 반복10/10 | 손실0.90\n",
            "|에폭177| 반복10/10 | 손실0.94\n",
            "|에폭178| 반복10/10 | 손실0.90\n",
            "|에폭179| 반복10/10 | 손실0.92\n",
            "|에폭180| 반복10/10 | 손실0.85\n",
            "|에폭181| 반복10/10 | 손실0.83\n",
            "|에폭182| 반복10/10 | 손실0.90\n",
            "|에폭183| 반복10/10 | 손실0.89\n",
            "|에폭184| 반복10/10 | 손실0.88\n",
            "|에폭185| 반복10/10 | 손실0.87\n",
            "|에폭186| 반복10/10 | 손실0.86\n",
            "|에폭187| 반복10/10 | 손실0.82\n",
            "|에폭188| 반복10/10 | 손실0.81\n",
            "|에폭189| 반복10/10 | 손실0.84\n",
            "|에폭190| 반복10/10 | 손실0.98\n",
            "|에폭191| 반복10/10 | 손실1.04\n",
            "|에폭192| 반복10/10 | 손실0.93\n",
            "|에폭193| 반복10/10 | 손실0.84\n",
            "|에폭194| 반복10/10 | 손실0.88\n",
            "|에폭195| 반복10/10 | 손실0.73\n",
            "|에폭196| 반복10/10 | 손실0.89\n",
            "|에폭197| 반복10/10 | 손실0.89\n",
            "|에폭198| 반복10/10 | 손실0.87\n",
            "|에폭199| 반복10/10 | 손실0.74\n",
            "|에폭200| 반복10/10 | 손실0.90\n",
            "|에폭201| 반복10/10 | 손실0.82\n",
            "|에폭202| 반복10/10 | 손실0.91\n",
            "|에폭203| 반복10/10 | 손실0.94\n",
            "|에폭204| 반복10/10 | 손실0.84\n",
            "|에폭205| 반복10/10 | 손실0.84\n",
            "|에폭206| 반복10/10 | 손실1.02\n",
            "|에폭207| 반복10/10 | 손실0.88\n",
            "|에폭208| 반복10/10 | 손실0.87\n",
            "|에폭209| 반복10/10 | 손실0.70\n",
            "|에폭210| 반복10/10 | 손실0.90\n",
            "|에폭211| 반복10/10 | 손실0.77\n",
            "|에폭212| 반복10/10 | 손실0.77\n",
            "|에폭213| 반복10/10 | 손실0.74\n",
            "|에폭214| 반복10/10 | 손실0.86\n",
            "|에폭215| 반복10/10 | 손실0.79\n",
            "|에폭216| 반복10/10 | 손실0.90\n",
            "|에폭217| 반복10/10 | 손실0.74\n",
            "|에폭218| 반복10/10 | 손실0.86\n",
            "|에폭219| 반복10/10 | 손실0.67\n",
            "|에폭220| 반복10/10 | 손실0.84\n",
            "|에폭221| 반복10/10 | 손실0.69\n",
            "|에폭222| 반복10/10 | 손실0.70\n",
            "|에폭223| 반복10/10 | 손실0.77\n",
            "|에폭224| 반복10/10 | 손실0.88\n",
            "|에폭225| 반복10/10 | 손실0.79\n",
            "|에폭226| 반복10/10 | 손실0.68\n",
            "|에폭227| 반복10/10 | 손실0.72\n",
            "|에폭228| 반복10/10 | 손실0.65\n",
            "|에폭229| 반복10/10 | 손실0.70\n",
            "|에폭230| 반복10/10 | 손실0.77\n",
            "|에폭231| 반복10/10 | 손실0.79\n",
            "|에폭232| 반복10/10 | 손실0.70\n",
            "|에폭233| 반복10/10 | 손실0.80\n",
            "|에폭234| 반복10/10 | 손실0.86\n",
            "|에폭235| 반복10/10 | 손실0.73\n",
            "|에폭236| 반복10/10 | 손실0.72\n",
            "|에폭237| 반복10/10 | 손실0.75\n",
            "|에폭238| 반복10/10 | 손실0.83\n",
            "|에폭239| 반복10/10 | 손실0.64\n",
            "|에폭240| 반복10/10 | 손실0.80\n",
            "|에폭241| 반복10/10 | 손실0.71\n",
            "|에폭242| 반복10/10 | 손실0.59\n",
            "|에폭243| 반복10/10 | 손실0.69\n",
            "|에폭244| 반복10/10 | 손실0.79\n",
            "|에폭245| 반복10/10 | 손실0.68\n",
            "|에폭246| 반복10/10 | 손실0.80\n",
            "|에폭247| 반복10/10 | 손실0.82\n",
            "|에폭248| 반복10/10 | 손실0.85\n",
            "|에폭249| 반복10/10 | 손실0.63\n",
            "|에폭250| 반복10/10 | 손실0.84\n",
            "|에폭251| 반복10/10 | 손실0.79\n",
            "|에폭252| 반복10/10 | 손실0.77\n",
            "|에폭253| 반복10/10 | 손실0.78\n",
            "|에폭254| 반복10/10 | 손실0.63\n",
            "|에폭255| 반복10/10 | 손실0.74\n",
            "|에폭256| 반복10/10 | 손실0.74\n",
            "|에폭257| 반복10/10 | 손실0.70\n",
            "|에폭258| 반복10/10 | 손실0.86\n",
            "|에폭259| 반복10/10 | 손실0.79\n",
            "|에폭260| 반복10/10 | 손실0.93\n",
            "|에폭261| 반복10/10 | 손실0.85\n",
            "|에폭262| 반복10/10 | 손실0.71\n",
            "|에폭263| 반복10/10 | 손실0.77\n",
            "|에폭264| 반복10/10 | 손실0.76\n",
            "|에폭265| 반복10/10 | 손실0.71\n",
            "|에폭266| 반복10/10 | 손실0.83\n",
            "|에폭267| 반복10/10 | 손실0.80\n",
            "|에폭268| 반복10/10 | 손실0.88\n",
            "|에폭269| 반복10/10 | 손실0.63\n",
            "|에폭270| 반복10/10 | 손실0.80\n",
            "|에폭271| 반복10/10 | 손실0.72\n",
            "|에폭272| 반복10/10 | 손실0.76\n",
            "|에폭273| 반복10/10 | 손실1.12\n",
            "|에폭274| 반복10/10 | 손실0.83\n",
            "|에폭275| 반복10/10 | 손실0.66\n",
            "|에폭276| 반복10/10 | 손실0.66\n",
            "|에폭277| 반복10/10 | 손실0.69\n",
            "|에폭278| 반복10/10 | 손실0.67\n",
            "|에폭279| 반복10/10 | 손실0.75\n",
            "|에폭280| 반복10/10 | 손실0.72\n",
            "|에폭281| 반복10/10 | 손실0.75\n",
            "|에폭282| 반복10/10 | 손실0.83\n",
            "|에폭283| 반복10/10 | 손실0.75\n",
            "|에폭284| 반복10/10 | 손실0.82\n",
            "|에폭285| 반복10/10 | 손실0.71\n",
            "|에폭286| 반복10/10 | 손실0.77\n",
            "|에폭287| 반복10/10 | 손실0.75\n",
            "|에폭288| 반복10/10 | 손실0.71\n",
            "|에폭289| 반복10/10 | 손실0.83\n",
            "|에폭290| 반복10/10 | 손실0.58\n",
            "|에폭291| 반복10/10 | 손실0.62\n",
            "|에폭292| 반복10/10 | 손실0.84\n",
            "|에폭293| 반복10/10 | 손실0.90\n",
            "|에폭294| 반복10/10 | 손실0.76\n",
            "|에폭295| 반복10/10 | 손실0.76\n",
            "|에폭296| 반복10/10 | 손실0.91\n",
            "|에폭297| 반복10/10 | 손실0.74\n",
            "|에폭298| 반복10/10 | 손실0.79\n",
            "|에폭299| 반복10/10 | 손실0.90\n",
            "|에폭300| 반복10/10 | 손실0.69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9qg3bYK5CCr",
        "colab_type": "text"
      },
      "source": [
        "학습을 수행하는 코드를 구현했다. \n",
        "\n",
        "1. 우선 하이퍼파라미터를 설정했다. 구체적으로는 학습하는 에폭 수, 미니배치 크기, 은닉층의 뉴런 수, 학습률을 설정했다.\n",
        "2. 계속해서 데이터를 읽어 들이고, 신경망(모델)과 옵티마이저를 생성했다. 미리 만들어둔 TwoLayerNet과 구현해둔 SGD 클래스를 불러와서 이용했다. 학습은 미니배치 방식으로 진행되었고, 데이터를 무작위로 선택했다.\n",
        "3. 에폭 단위로 데이터를 뒤섞고, 뒤섞은 데이터 중 앞에서부터 순서대로 뽑아내는 방식을 사용했다. 데이터의 인덱스를 뒤섞을 때 np.random.permutation() 메서드를 사용했다. 이 메서드에 인수로 N을 주면, 0에서부터 N-1까지의 무작위 순서를 생성해 반환한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f1n1RFu4SEz",
        "colab_type": "code",
        "outputId": "c901050f-bc90-4d97-d765-9450d4370fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import numpy as np\n",
        "np.random.permutation(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 8, 4, 9, 7, 0, 2, 6, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLw-vMmT6_jn",
        "colab_type": "code",
        "outputId": "752dcaa7-3dce-4c00-905b-d4a9f8415b74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "np.random.permutation(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 4, 2, 7, 8, 5, 6, 0, 9, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYpZczAo7Eel",
        "colab_type": "text"
      },
      "source": [
        "np.random.permutation() 메서드를 사용해보았다. 이처럼 이 메서드를 호출하면 데이터 인덱스를 무작위로 뒤섞을 수 있다.\n",
        "\n",
        " 4. 계속해서 기울기를 구해 매개변수를 갱신했다.\n",
        " 5. 마지막으로, 정기적으로 학습 결과를 출력했다. 10번째 반복마다 손실의 평균을 구해 loss_list 변수에 추가하도록 했다.\n",
        "\n",
        "결과를 보면 학습을 진행함에 따라 손실이 줄어들고 있음을 확인할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqTE4qPn7vsk",
        "colab_type": "text"
      },
      "source": [
        "### **1.4.4 Trainer 클래스**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z8HtFpX9aNK",
        "colab_type": "text"
      },
      "source": [
        "교재에서는 학습을 수행하는 역할을 Trainer 클래스로 제공한다. Trainer 클래스는 아래와 같이 사용하면 된다.\n",
        "\n",
        "model = TwoLayerNet(...)\n",
        "\n",
        "optimizer = SGD(lr=1.0)\n",
        "\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "\n",
        "___\n",
        "그리고 fit() 메서드를 호출해 학습을 시작하면 되는데, fit() 메서드가 받는 인수는 다음과 같다.\n",
        "\n",
        " 1. x: 입력 데이터\n",
        " 2. t: 정답 레이블\n",
        " 3. max_epoch: 학습을 수행하는 에폭 수\n",
        " 4. batch_size: 미니배치 크기 \n",
        " 5. eval_interval: 결과(평균 손실 등)를 출력하는 간격\n",
        " 6. max_grad: 기울기 최대 norm (기울기 norm이 이 값을 넘어서면 기울기를 줄인다. 이를 기울기 클리핑이라고 한다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1zAbDD09Qhf",
        "colab_type": "code",
        "outputId": "58d0e13e-0683-4f89-aaf0-e4ea7678411c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.optimizer import SGD\n",
        "from common.trainer import Trainer\n",
        "from dataset import spiral\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "max_epoch = 300\n",
        "batch_size = 30\n",
        "hidden_size = 10\n",
        "learning_rate = 1.0\n",
        "x,t = spiral.load_data()\n",
        "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
        "optimizer = SGD(lr=learning_rate)\n",
        "\n",
        "trainer = Trainer(model,optimizer)\n",
        "trainer.fit(x,t,max_epoch,batch_size,eval_interval=10)\n",
        "trainer.plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 2 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 3 |  반복 1 / 10 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 4 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 5 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 6 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 7 |  반복 1 / 10 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 8 |  반복 1 / 10 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 9 |  반복 1 / 10 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 10 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 11 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 12 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 13 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 14 |  반복 1 / 10 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 15 |  반복 1 / 10 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 16 |  반복 1 / 10 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 17 |  반복 1 / 10 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 18 |  반복 1 / 10 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 19 |  반복 1 / 10 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 20 |  반복 1 / 10 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 21 |  반복 1 / 10 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 22 |  반복 1 / 10 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 23 |  반복 1 / 10 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 24 |  반복 1 / 10 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 25 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 26 |  반복 1 / 10 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 27 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 28 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 29 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 30 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 31 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 32 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 33 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 34 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 35 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 36 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 37 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 38 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 39 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 40 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 41 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 42 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 43 |  반복 1 / 10 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 44 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 45 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 46 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 47 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 48 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 49 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 50 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 51 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 52 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 53 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 54 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 55 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 56 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 57 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 58 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 59 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 60 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 61 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 62 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 63 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 64 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 65 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 66 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 67 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 68 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 69 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 70 |  반복 1 / 10 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 71 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 72 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 73 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 74 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 75 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 76 |  반복 1 / 10 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 77 |  반복 1 / 10 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 78 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 79 |  반복 1 / 10 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 80 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 81 |  반복 1 / 10 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 82 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 83 |  반복 1 / 10 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 84 |  반복 1 / 10 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 85 |  반복 1 / 10 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 86 |  반복 1 / 10 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 87 |  반복 1 / 10 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 88 |  반복 1 / 10 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 89 |  반복 1 / 10 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 90 |  반복 1 / 10 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 91 |  반복 1 / 10 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 92 |  반복 1 / 10 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 93 |  반복 1 / 10 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 94 |  반복 1 / 10 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 95 |  반복 1 / 10 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 96 |  반복 1 / 10 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 97 |  반복 1 / 10 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 98 |  반복 1 / 10 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 99 |  반복 1 / 10 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 100 |  반복 1 / 10 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 101 |  반복 1 / 10 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 102 |  반복 1 / 10 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 103 |  반복 1 / 10 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 104 |  반복 1 / 10 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 105 |  반복 1 / 10 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 106 |  반복 1 / 10 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 107 |  반복 1 / 10 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 108 |  반복 1 / 10 | 시간 0[s] | 손실 0.39\n",
            "| 에폭 109 |  반복 1 / 10 | 시간 0[s] | 손실 0.40\n",
            "| 에폭 110 |  반복 1 / 10 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 111 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 112 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 113 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 114 |  반복 1 / 10 | 시간 0[s] | 손실 0.37\n",
            "| 에폭 115 |  반복 1 / 10 | 시간 0[s] | 손실 0.36\n",
            "| 에폭 116 |  반복 1 / 10 | 시간 0[s] | 손실 0.34\n",
            "| 에폭 117 |  반복 1 / 10 | 시간 0[s] | 손실 0.35\n",
            "| 에폭 118 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
            "| 에폭 119 |  반복 1 / 10 | 시간 0[s] | 손실 0.35\n",
            "| 에폭 120 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
            "| 에폭 121 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
            "| 에폭 122 |  반복 1 / 10 | 시간 0[s] | 손실 0.32\n",
            "| 에폭 123 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
            "| 에폭 124 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
            "| 에폭 125 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
            "| 에폭 126 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
            "| 에폭 127 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
            "| 에폭 128 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
            "| 에폭 129 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
            "| 에폭 130 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
            "| 에폭 131 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 132 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
            "| 에폭 133 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
            "| 에폭 134 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
            "| 에폭 135 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 136 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
            "| 에폭 137 |  반복 1 / 10 | 시간 0[s] | 손실 0.25\n",
            "| 에폭 138 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 139 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 140 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 141 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 142 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 143 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 144 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 145 |  반복 1 / 10 | 시간 0[s] | 손실 0.24\n",
            "| 에폭 146 |  반복 1 / 10 | 시간 0[s] | 손실 0.24\n",
            "| 에폭 147 |  반복 1 / 10 | 시간 0[s] | 손실 0.25\n",
            "| 에폭 148 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 149 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 150 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 151 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 152 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 153 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 154 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 155 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 156 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 157 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 158 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 159 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 160 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 161 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 162 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 163 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 164 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 165 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 166 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 167 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 168 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 169 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 170 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 171 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 172 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 173 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 174 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 175 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 176 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 177 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 178 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 179 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 180 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 181 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 182 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 183 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 184 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 185 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 186 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 187 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 188 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 189 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 190 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 191 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 192 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 193 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 194 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 195 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 196 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 197 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 198 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 199 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 200 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 201 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 202 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 203 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 204 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 205 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 206 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 207 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 208 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 209 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 210 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 211 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 212 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 213 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 214 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 215 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 216 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 217 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 218 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 219 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 220 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 221 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 222 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 223 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 224 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 225 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 226 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 227 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 228 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 229 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 230 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 231 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 232 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 233 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 234 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 235 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 236 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 237 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 238 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 239 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 240 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 241 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 242 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 243 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 244 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 245 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 246 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 247 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 248 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 249 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 250 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 251 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 252 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 253 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 254 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 255 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 256 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 257 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 258 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 259 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 260 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 261 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 262 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 263 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 264 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 265 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 266 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 267 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 268 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 269 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 270 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 271 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 272 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 273 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 274 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 275 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 276 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 277 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 278 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 279 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 280 |  반복 1 / 10 | 시간 0[s] | 손실 0.10\n",
            "| 에폭 281 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 282 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 283 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 284 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 285 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 286 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 287 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 288 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 289 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 290 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 291 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 292 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 293 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 294 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 295 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 296 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 297 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 298 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 299 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 300 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEJCAYAAACZjSCSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngVJKBTgNAqWUCnD/H8l3t2Q0JckTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xh8J1-C_G7g",
        "colab_type": "text"
      },
      "source": [
        "Trainer 클래스를 사용해 학습을 수행했다. 결과에서 손실이 줄어드는 것을 확인했으며, fit()에서 기록한 손실을 plot() 메서드를 이용해 그래프로 그려주었다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvmx5hJH_X8_",
        "colab_type": "text"
      },
      "source": [
        "## **1.5 계산 고속화**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYHzhzsc_lDy",
        "colab_type": "text"
      },
      "source": [
        "### **1.5.1 비트 정밀도**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgOY70cG-7rG",
        "colab_type": "code",
        "outputId": "ddb40b78-6fdf-475c-e956-4b20b022cc22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "a = np.random.randn(3)\n",
        "a.dtype"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngQLnJ9n_tuA",
        "colab_type": "text"
      },
      "source": [
        "넘파이의 부동소수점 수는 기본적으로 64비트 데이터 타입을 사용한다. 위의 코드로 실제로 64비트 부동소수점 수가 사용됨을 확인했다.\n",
        "\n",
        "그러나 신경망의 추론과 학습은 32비트 부동소수점 수로도 문제없이 수행할 수 있다. 32비트는 64비트의 절반이므로 메모리 관점에서는 항상 32비트가 더 좋다. 또, 신경망 계산 시 데이터를 전송하는 버스 대역폭이 병목이 되는 경우가 종종 있다. 이 경우에도 데이터 타입이 작은 것이 유리하다. 마지막으로 계산 속도 측면에서도 32비트 부동소수점 수가 일반적으로 더 빠르다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DyXr5Eo_sp_",
        "colab_type": "code",
        "outputId": "d9b08233-eada-4134-b33c-1d4cbc05c080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "b = np.random.randn(3).astype(np.float32)\n",
        "b.dtype"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2O-Km_dASUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cda18c34-8753-4fca-e276-2e25cdd39038"
      },
      "source": [
        "c = np.random.randn(3).astype('f')\n",
        "c.dtype"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alBXc05tht3l",
        "colab_type": "text"
      },
      "source": [
        "넘파이에서 32비트 부동소수점 수를 사용하려면 위와 같이 데이터 타입을 np.float32나 f로 지정하면 된다. \n",
        "\n",
        "또한 신경망 추론으로 한정하면, 16비트 부동소수점 수를 사용해도 인식률이 거의 떨어지지 않는다. 넘파이에도 16비트 부동소수점 수가 준비되어 있다. 다만, 일반적으로 CPU와 GPU는 연산 자체를 32비트로 수행하기 때문에 16비트 부동소수점 수로 변환하더라도 계산 자체는 32비트로 이루어져 처리 속도에는 큰 차이가 없을 수 있다. 그러나 학습된 가중치를 저장하는 경우에는 16비트 부동소수점 수가 여전히 유효하다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYRZ-8ZoiZ35",
        "colab_type": "text"
      },
      "source": [
        "### **1.5.2 GPU(쿠파이)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EujfO1icigjm",
        "colab_type": "text"
      },
      "source": [
        "딥러닝의 계산은 대량의 곱하기 연산으로 구성된다. 이 대량의 곱하기 연산 대부분은 병렬로 계산할 수 있는데, 이 점에서 CPU보다 GPU가 유리하다. \n",
        "\n",
        "쿠파이는 GPU를 이용해 병렬 계산을 수행해주는 라이브러리인데, 아쉽게도 엔비디아의 GPU에서만 동작한다. 또한, CUDA라는 GPU 전용 범용 병렬 컴퓨팅 플랫폼을 설치해야 한다. 쿠파이를 사용하면 간단하게 병렬 계산을 수행할 수 있다. 또한 쿠파이는 넘파이와 호환되는 API를 제공한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuiYEMF2jiom",
        "colab_type": "text"
      },
      "source": [
        "import cupy as cp\n",
        "\n",
        "x = cp.arange(6).reshape(2,3).astype('f')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEseSvySjla5",
        "colab_type": "text"
      },
      "source": [
        "이것이 쿠파이의 사용법이다. 사용법이 기본적으로 넘파이와 동일하다는 것을 알 수 있다. 다시 말해, 넘파이로 작성한 코드를 'GPU용'으로 변경하기가 아주 쉽다는 뜻이다. 그저 numpy를 cupy로 대체해주기만 하면 끝이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD7TTJjCjIYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.arange(6).reshape(2,3).astype('f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTeG5XLUjao4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b1bd8a30-1c17-48d2-ff06-2a4c4f84511e"
      },
      "source": [
        "x"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 2.],\n",
              "       [3., 4., 5.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nChi0Irjbsx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c89c051a-75d6-451e-948c-2093f2e88a97"
      },
      "source": [
        "x.sum(axis=1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3., 12.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}